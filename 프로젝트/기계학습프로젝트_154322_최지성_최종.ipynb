{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fas_train = pd.read_csv('archive/fashion-mnist_train.csv')\n",
    "fas_test = pd.read_csv('archive/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ln 3 to ln 7 : https://www.kaggle.com/egenaz/fashion-mnist-modeling-w-randomforestclassifier 부분 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idx2numpy in c:\\anaconda\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy in c:\\anaconda\\lib\\site-packages (from idx2numpy) (1.20.1)\n",
      "Requirement already satisfied: six in c:\\anaconda\\lib\\site-packages (from idx2numpy) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install idx2numpy\n",
    "import idx2numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'archive/t10k-images-idx3-ubyte'\n",
    "array = idx2numpy.convert_from_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAPSElEQVR4nO3dbYiV95nH8d/lw2h8iOhajUllrSYhhjxMgwwmGZYsZUsqBCOhSw00bgidvmhJC4VsyL6obxbCsm23L5bCdBNqQ1cpVFHysNSIoMVQNGLNpGY3aoxOHZzGMXF8HsdrX8ydMKNz/+/juc+TXt8PDOfMfZ37nMvD/LzPOf9z///m7gJw85vQ7AYANAZhB4Ig7EAQhB0IgrADQUxq5IOZGR/9V2HmzJnJ+rx583Jr58+fT+47aVL6T+DixYvJ+sSJE6uuF40ETZkyJVk/dOhQsh6Vu9t420uF3cwel/RzSRMl/Ze7v1zm/m5WZuM+918o+qPv6OhI1p9//vnc2r59+5L73nbbbcn6wYMHk/UZM2Yk67Nnz86tDQ0NJfddvHhxsr5q1apkHWNV/TLezCZK+k9J35B0r6TVZnZvrRoDUFtl3rN3SDro7ofd/ZKkDZJW1qYtALVWJux3SDo26vfebNsYZtZlZnvMbE+JxwJQUpn37OO9Eb3mzae7d0vqlviADmimMkf2XkkLR/3+ZUnHy7UDoF7KhH23pLvM7Ctm1ibpW5K21KYtALVmZc56M7MVkv5DI0Nvr7r7vxbcPuTL+AkT0v+nXrlyJVnfuXNnst7Z2XndPVXq9OnTyfq0adOS9dQ4/rlz50rd9xNPPJGsv/7668n6zaou4+zu/qakN8vcB4DG4OuyQBCEHQiCsANBEHYgCMIOBEHYgSAaej57VEXj6EXa29uT9YGBgdzaJ598kty3zDi5JJ08eTJZv3z5cm6t6NTfO++8M1m/5557kvWo4+x5OLIDQRB2IAjCDgRB2IEgCDsQBGEHgmDo7QZQNINranjt1ltvTe5bdPpt2amkU9NBF913kYULFxbfCF/gyA4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQTDO3gLmz59fav/UaqhFU4UXjbMXjaOnTmGV0qf3FvVWNI11aqlqXIsjOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwTh7C7jvvvtK7Z8aZ7/llluS+w4PD5eqF43TpxSN4Red7z537tyqHzuiUmE3syOSBiUNS7rs7stq0RSA2qvFkf3v3T29EgGApuM9OxBE2bC7pN+b2btm1jXeDcysy8z2mNmeko8FoISyL+MfdffjZjZP0lYz+8Ddd4y+gbt3S+qWJDNLn/kAoG5KHdnd/Xh22S9pk6SOWjQFoPaqDruZTTezmZ9fl/R1ST21agxAbZV5GT9f0qZs2d1Jkv7b3f+nJl0F88ADDyTrly5dStYvXLiQWytakjk1r7tUPO98arnoIkVLNhf1dvbs2aofO6Kqw+7uhyU9WMNeANQRQ29AEIQdCIKwA0EQdiAIwg4EwSmuLaCjI/1dpNR0zFJ6eK1oqudZs2Yl63v37k3W29vbk/VTp07l1opOYS0aNjx27FiyjrE4sgNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIyzt4ClS5cm66mpoqX0OPyMGTOS+/b19SXry5cvT9bLLAldNA31pEnpP88yp9dGxJEdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnL0FFJ1TXnROeplx9o0bNybrZaWWZS5aDrpIW1tbqf2j4cgOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzt4C5s2bl6yfO3cuWS86pzxl/fr1Ve8rFc/9PmfOnNzayZMnSz120bzyGKvwyG5mr5pZv5n1jNo2x8y2mtmH2eXs+rYJoKxKXsb/StLjV217UdI2d79L0rbsdwAtrDDs7r5D0tXz/6yUtC67vk7Sk7VtC0CtVfuefb6790mSu/eZWe6bTjPrktRV5eMAqJG6f0Dn7t2SuiXJzKr/JAlAKdUOvZ0wswWSlF32164lAPVQbdi3SFqTXV8jaXNt2gFQL4Uv481svaTHJM01s15JP5b0sqTfmtlzko5K+mY9m7zZFY0XnzlzJlkvml89Zfv27VXvK0nvvPNOsv7www/n1lLnulei7Dh9NIV/Je6+Oqf0tRr3AqCO+LosEARhB4Ig7EAQhB0IgrADQXCK601g8uTJubWiaaiLTlEtcuTIkWS9s7Mzt2ZmpR77s88+K7V/NBzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtlvAEVTRafG2Q8dOlTrdsbo7e1N1idMyD+elJkCG9ePIzsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBME4+w1gaGgoWZ8+fXpuraenJ7dWC2+88Uay/sILL+TWUmPwqD2ebSAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgnH2G0CZpY0/+uijGnZyrf379yfrbW1tubXUefiVOHv2bKn9oyk8spvZq2bWb2Y9o7atNbO/mNm+7GdFfdsEUFYlL+N/Jenxcbb/zN3bs583a9sWgForDLu775A00IBeANRRmQ/ovm9m+7OX+bPzbmRmXWa2x8z2lHgsACVVG/ZfSFoiqV1Sn6Sf5N3Q3bvdfZm7L6vysQDUQFVhd/cT7j7s7lck/VJSR23bAlBrVYXdzBaM+nWVpPqeRwmgtMJxdjNbL+kxSXPNrFfSjyU9ZmbtklzSEUnfrV+LN7+iudenTZuWrKfmXz9+/HhVPVWqaP33lDLfH5AYZ79ehWF399XjbH6lDr0AqCO+LgsEQdiBIAg7EARhB4Ig7EAQnOLaAk6cOJGsL1myJFlPDWHdfffdVfVUqUuXLlW97/DwcKnHLhqSxFgc2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZW8Du3buT9aVLlybrFy9ezK09+OCDVfXUCFOmTCm1f+rfjWtxZAeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnbwE7duxI1p999tlkfWhoKLf20EMPVdVTraTOWS87lXTZ8+Gj4cgOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0Ewzt4Cdu3alaxfuHAhWU8tm9zf319VT7UyODiYWzOzUvdddpw+msIju5ktNLPtZnbAzN43sx9k2+eY2VYz+zC7nF3/dgFUq5KX8Zcl/cjdl0paLul7ZnavpBclbXP3uyRty34H0KIKw+7ufe6+N7s+KOmApDskrZS0LrvZOklP1qlHADVwXe/ZzWyRpK9K+qOk+e7eJ438h2Bm83L26ZLUVbJPACVVHHYzmyHpd5J+6O6nK/1wxd27JXVn9+HVNAmgvIqG3sxsskaC/ht335htPmFmC7L6AknN/dgXQFLhkd1GDuGvSDrg7j8dVdoiaY2kl7PLzXXpMICPP/44WT99+nSynpqSeerUqcl9Fy9enKwfPnw4WS+SOv120qRyI78MvV2fSp7tRyV9W9J7ZrYv2/aSRkL+WzN7TtJRSd+sS4cAaqIw7O7+B0l5b9C/Vtt2ANQLX5cFgiDsQBCEHQiCsANBEHYgCE5xvQEULW2cGm9ua2tL7lvvcfa+vr7c2qJFi5L7DgwMJOsTJnCsuh48W0AQhB0IgrADQRB2IAjCDgRB2IEgCDsQBOPsDVA0q497egKfTZs2JetPP/10bq1oLLqzszNZf/vtt5P1ImfPnq1636Ln7dNPP636viPiyA4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQTDO3gBlx9k3b05Pyf/MM8/k1lLztkvSU089layvXbs2WS+Smhu+6N9dVC9ayhpjcWQHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAqWZ99oaRfS7pN0hVJ3e7+czNbK+k7kv6a3fQld3+zXo3eyIrOKb9y5Uqy/tZbbyXrp06dyq0VzTlf9Nhl9fT05Nbuv//+5L7nz59P1m+//faqeoqqki/VXJb0I3ffa2YzJb1rZluz2s/c/d/r1x6AWqlkffY+SX3Z9UEzOyDpjno3BqC2rus9u5ktkvRVSX/MNn3fzPab2atmNjtnny4z22Nme8q1CqCMisNuZjMk/U7SD939tKRfSFoiqV0jR/6fjLefu3e7+zJ3X1a+XQDVqijsZjZZI0H/jbtvlCR3P+Huw+5+RdIvJXXUr00AZRWG3UZO2XpF0gF3/+mo7QtG3WyVpPyPXQE0XSWfxj8q6duS3jOzfdm2lyStNrN2SS7piKTv1qG/m8Lw8HBd7//o0aO5teXLlyf3nT59erL+yCOPJOu7du1K1lPLSU+dOjW57+TJk5P1uXPnJusYq5JP4/8gabwTshlTB24gfIMOCIKwA0EQdiAIwg4EQdiBIAg7EARTSTdA0ZTIZXV3d+fWPvjgg+S+GzZsSNaLxtGLvPbaa7m1WbNmJfcdHBxM1nfu3FlVT1FxZAeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIKzeY8BjHszsr5I+HrVprqRPGtbA9WnV3lq1L4neqlXL3v7W3b80XqGhYb/mwc32tOrcdK3aW6v2JdFbtRrVGy/jgSAIOxBEs8Oe/6Xu5mvV3lq1L4neqtWQ3pr6nh1A4zT7yA6gQQg7EERTwm5mj5vZ/5rZQTN7sRk95DGzI2b2npnta/b6dNkaev1m1jNq2xwz22pmH2aX466x16Te1prZX7Lnbp+ZrWhSbwvNbLuZHTCz983sB9n2pj53ib4a8rw1/D27mU2U9H+S/kFSr6Tdkla7+58b2kgOMzsiaZm7N/0LGGb2d5LOSPq1u9+Xbfs3SQPu/nL2H+Vsd//nFultraQzzV7GO1utaMHoZcYlPSnpn9TE5y7R1z+qAc9bM47sHZIOuvthd78kaYOklU3oo+W5+w5JA1dtXilpXXZ9nUb+WBoup7eW4O597r43uz4o6fNlxpv63CX6aohmhP0OScdG/d6r1lrv3SX93szeNbOuZjczjvnu3ieN/PFImtfkfq5WuIx3I121zHjLPHfVLH9eVjPCPt5SUq00/veouz8k6RuSvpe9XEVlKlrGu1HGWWa8JVS7/HlZzQh7r6SFo37/sqTjTehjXO5+PLvsl7RJrbcU9YnPV9DNLvub3M8XWmkZ7/GWGVcLPHfNXP68GWHfLekuM/uKmbVJ+pakLU3o4xpmNj374ERmNl3S19V6S1FvkbQmu75G0uYm9jJGqyzjnbfMuJr83DV9+XN3b/iPpBUa+UT+kKR/aUYPOX0tlvSn7Of9Zvcmab1GXtYNaeQV0XOS/kbSNkkfZpdzWqi31yS9J2m/RoK1oEm9dWrkreF+SfuynxXNfu4SfTXkeePrskAQfIMOCIKwA0EQdiAIwg4EQdiBIAg7EARhB4L4f/aYpQGTCBAmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAATHElEQVR4nO3dW2xV55UH8P+C4ISbBQZjm4vBKTgaMkkMImgUyGVSTRV4COGho/IQZZJoXEWt1EgVMyjz0LxMhGam0+nDqIo7RaVRJxVKG4VIZARClRjy0MSJGEOCCdeAsbEBBzCXBIzXPHgTOcR7Lefsc84+9fr/JGT7LH8+y9tnsc85a3/fJ6oKIhr/JuSdABGVB4udKAgWO1EQLHaiIFjsREHcUc47ExG+9V8CU6dOTY1NmGD/fy4imeIea/zg4KA59vLly5nuOypVHfWgZyp2EXkCwM8BTATwX6q6OcvPy8J7UA8NDZnxLA/qvNuX9913X2rM+o8AAKqqqsz4xIkTC8rpljvvvDM1dvbsWXPsnj17Mt03fVXBT+NFZCKA/wSwBsBSABtEZGmxEiOi4srymn0lgCOqekxVrwP4HYB1xUmLiIotS7HPA3BqxNddyW1fISKtItIuIu0Z7ouIMsrymn20F7lfe/Gqqm0A2gC+QUeUpyxn9i4AC0Z8PR9Ad7Z0iKhUshT7+wCWiEiTiFQB+B6A7cVJi4iKTbK0jURkLYD/wHDrbYuq/rPz/SV7Gp+1H+zJcpymT59uxh9//HEzvnz5cjO+Zs2a1NihQ4fMsd7vNW3aNDM+a9YsM37u3LnU2OTJk82xXtvv7bffNuPbt6efe06ePGmO/XNWkj67qu4AsCPLzyCi8uDlskRBsNiJgmCxEwXBYicKgsVOFASLnSiITH32b3xnOV4u6/XhsxyH1tZWM97c3GzGvX5yZ2enGbd64S0tLebYzz//3Ix7U2S9OeeXLl1KjV29etUcW1tbm+m+m5qaCr7vTZs2mfHu7sq9WDStz84zO1EQLHaiIFjsREGw2ImCYLETBcFiJwpi3LTeStlaA4AXXnghNeZN87xw4YIZv3Hjhhn3Vs61WlDW6q4AsH79ejN+5swZM+61sKzW3nvvvWeOtabuAsD+/fvNuNU2XLhwoTnWa4c+99xzZjxPbL0RBcdiJwqCxU4UBIudKAgWO1EQLHaiIFjsREGUdcvmUsraZ1+wYIEZb2xsTI0dO3bMHOstx+y5cuWKGa+rq0uNHT161Bzr5b5kyRIzfv78eTNu9dIfeeQRc+zp06fN+F133WXGraWqr127Zo6tr683408//bQZf+2118y49Xgt1bUvPLMTBcFiJwqCxU4UBIudKAgWO1EQLHaiIFjsREGMmz770NBQpvGLFy8244ODg6mxO+6wD6O35LE359ybW239/BkzZphjd+ywN+F95ZVXzLjXr7aOjXfcent7zbi3zHV1dXVqrKqqyhz7xRdfmPFly5aZca/PXs51JG7JVOwicgLAAICbAAZVdUUxkiKi4ivGmf2vVfVcEX4OEZUQX7MTBZG12BXAThH5QERG3QNJRFpFpF1E2jPeFxFlkPVp/CpV7RaROQB2iUinqu4Z+Q2q2gagDch3rzei6DKd2VW1O/nYB+BNACuLkRQRFV/BxS4iU0Vk+q3PAXwHwIFiJUZExZXlaXwdgDeTebl3APhvVf2fomSVg3vvvdeMW+ufe31yjzdf3euz37x5MzVm9ZoBoKenx4zv3LnTjFvXHwB2bkeOHDHHemsUeHPOrT6+Nxfe8+CDD2Yan4eCi11VjwF4oIi5EFEJsfVGFASLnSgIFjtRECx2oiBY7ERBjJsprlnNnz/fjF+8eDE1lrX11tfXZ8anTJlixq0W0/Xr182xXsuxo6PDjNfU1Jjx7u7u1NjcuXPNsd70XGsJbcBuK3q/9/Hjx814f3+/Gfem0Hp/l1LgmZ0oCBY7URAsdqIgWOxEQbDYiYJgsRMFwWInCiJMn93ryXqsbZdnzpxpjvV61Tdu3DDj3hRXi7fEtrdksve7ef1ka5qqt5R0Q0ODGfdyt3LzevieCRPs8+T9999vxtvby79KG8/sREGw2ImCYLETBcFiJwqCxU4UBIudKAgWO1EQYfrsTU1NZjzLtsre1sHe9rzenPBJkyaZ8SzLInv9YmspaMDv49fW1n7jnG7x1gnw+vTWOgADAwOZ7ttbQtt7vLHPTkQlw2InCoLFThQEi50oCBY7URAsdqIgWOxEQYTpszc2Nppxa0tmwO9HZ7nvTz/91Ix7a4xb8929ufDe9QVeP9n73az79362N189y3z4q1evmmO9NQa8eHNzsxnPg/sIFpEtItInIgdG3FYjIrtE5HDy0V7hgIhyN5bT1a8BPHHbbZsA7FbVJQB2J18TUQVzi11V9wC4fa+bdQC2Jp9vBfBUcdMiomIr9DV7nar2AICq9ojInLRvFJFWAK0F3g8RFUnJ36BT1TYAbQAgIvaMECIqmULfYu4VkQYASD7a25ASUe4KLfbtAJ5JPn8GwFvFSYeISsV9Gi8irwN4DMBsEekC8BMAmwFsE5HnAZwE8N1SJlkM3l7g3rztS5cupca8uc/V1dVm3JsT7vWTrdy9Prs319773byfb80b99ak9659mDx5shm3/mazZ882x164cMGMe9ddtLS0mPE8uMWuqhtSQt8uci5EVEK8XJYoCBY7URAsdqIgWOxEQbDYiYIIM8XV2nIZ8KeRfvbZZ6kxb5rnW2/ZlyF4uXntMWu6pdc68+LeMtbeVE+rbegtge21JL3WXGdnZ2rsySefNMd6x9x7vGRZ3rtUeGYnCoLFThQEi50oCBY7URAsdqIgWOxEQbDYiYII02f3+snXrl0z49ayxyJijv3444/N+MMPP2zGveWeLd7U3RkzZphx6/oCwO9HW8fN69F7x9XzySefpMas7ZzHct/eMtfecc0Dz+xEQbDYiYJgsRMFwWInCoLFThQEi50oCBY7URDjps/uLbdcVVVlxr0lkS1ev7i7u9uMZ+0nW0sqe332qVOnmvHz58+bca/PbsWz9tm9v9nhw4dTY16f3Vsq2nu8ecfVWsMgy3UVFp7ZiYJgsRMFwWInCoLFThQEi50oCBY7URAsdqIgxk2f3duC1+vZev1iq6/qrSHu9WS9uDUnHLDn6vf395tjr169asa9deO9bZP7+vpSY941AN7fzBvf09NT8FiPt/6B93iqr69PjR05cqSgnDzumV1EtohIn4gcGHHbyyJyWkT2Jf/WliQ7IiqasTyN/zWAJ0a5/Weq2pL821HctIio2NxiV9U9AOzngkRU8bK8QfdDEelInubPTPsmEWkVkXYRac9wX0SUUaHF/gsA3wLQAqAHwE/TvlFV21R1haquKPC+iKgICip2Ve1V1ZuqOgTglwBWFjctIiq2gopdRBpGfLkewIG07yWiyuD22UXkdQCPAZgtIl0AfgLgMRFpAaAATgD4fulSHBtvnW6vl+3t9W39/FOnTpljBwYGzLg39/nMmTNm3PrdvHnZXr/Z22fc67NbP9+7fsD7m3n72ltxq/8P+HvDe7l5x33OnDmpsVL12d1iV9UNo9z8qxLkQkQlxMtliYJgsRMFwWInCoLFThQEi50oiHEzxdWbUugtz+ttwdvc3Jwa6+zszHTfXgvKYy2p7E1R9Y6b15L0pnparTuvPeWpqakx41euXEmN7d+/3xw7ffp0M+5tZe217ry2YSnwzE4UBIudKAgWO1EQLHaiIFjsREGw2ImCYLETBTFu+uyzZs0y414v25uqaU1x7ejoMMfW1taaca+n67GmW1rLTAN2Lxrwp8B6/WTruHpbLntbOnv33djYmBo7evSoOfahhx4y497jxbv2orq62oyXAs/sREGw2ImCYLETBcFiJwqCxU4UBIudKAgWO1EQ46bPvnz5cjPu9UW9eF1dXWrMm9u8YoW9GY63bbLXT7biXi/b227aG+/FrTnr3hoCXty7duKBBx5IjV28eNEcm2WePuAvD249Jt544w1zbKF4ZicKgsVOFASLnSgIFjtRECx2oiBY7ERBsNiJghg3fXZvXrbXF503b54Zt+ac79u3zxzb0tJixi9cuGDGp0yZYsYtImLGvfnuXh/dm+9u/V28Hr/XR/euP1i0aFFqbPv27ebYLVu2mPFt27aZce/x2NPTY8ZLwT2zi8gCEfmjiBwUkY9E5EfJ7TUisktEDicfZ5Y+XSIq1Fiexg8C+LGq/gWAvwLwAxFZCmATgN2qugTA7uRrIqpQbrGrao+qfph8PgDgIIB5ANYB2Jp821YAT5UoRyIqgm/0ml1EFgFYBuBPAOpUtQcY/g9BROakjGkF0JoxTyLKaMzFLiLTAPwewIuqesl74+cWVW0D0Jb8DHsXQSIqmTG13kRkEoYL/beq+ofk5l4RaUjiDQD6SpMiERWDeFv2yvApfCuAflV9ccTt/wrgvKpuFpFNAGpU9R+cn1WxZ3ZvC9277747NXbgwAFz7MaNG824N0XWa49ZyxKfPn3aHDt79mwz7m357B23rq4uM25paGgw43PmjPrK8UtNTU2psWeffdYc67UFvW24va2uS0lVR33aPZan8asAPA1gv4jsS257CcBmANtE5HkAJwF8twh5ElGJuMWuqnsBpL1A/3Zx0yGiUuHlskRBsNiJgmCxEwXBYicKgsVOFMS4meKaldc3tbZl9rZc9raT7u/vN+PWlswA0Nvbmxrzlsj2cvOulPT60dZ1HN71A95S0h5rarC1zDQAvPPOO5nuuxLxzE4UBIudKAgWO1EQLHaiIFjsREGw2ImCYLETBRGmz+71i62thQF7yeTVq1ebY2/cuGHGPd72wVbuixcvNsceP368oJxusbayBuzj7i3v7W1l7R0Xay7/o48+ao71+uze48lbJyIPPLMTBcFiJwqCxU4UBIudKAgWO1EQLHaiIFjsREGE6bN7fU9v62HLPffcY8YvXrxoxquqqsy4l1tzc3Nq7MSJE+ZYb2vhuXPnmnGvV25dA+DNtc86l96K19fXm2M9Y9hvIdP4UuCZnSgIFjtRECx2oiBY7ERBsNiJgmCxEwXBYicKwu2zi8gCAL8BUA9gCECbqv5cRF4G8PcAzibf+pKq7ihVoqU2ceJEM271uhcuXGiO9frohw8fNuNDQ0Nm/NChQ6kxb036pUuXZrpvb/9267gNDAyYY7Nen2CtS2+tKe+NBfw17Suxzz6Wi2oGAfxYVT8UkekAPhCRXUnsZ6r6b6VLj4iKZSz7s/cA6Ek+HxCRgwDmlToxIiqub/SaXUQWAVgG4E/JTT8UkQ4R2SIiM1PGtIpIu4i0Z0uViLIYc7GLyDQAvwfwoqpeAvALAN8C0ILhM/9PRxunqm2qukJVV2RPl4gKNaZiF5FJGC7036rqHwBAVXtV9aaqDgH4JYCVpUuTiLJyi12G31b8FYCDqvrvI25vGPFt6wEcKH56RFQsMoapeqsB/C+A/RhuvQHASwA2YPgpvAI4AeD7yZt51s+qvPV1E95S0lYLypuquXHjRjO+atUqMz5jxgwzbi0H7S1j7eV+9uxZMz5z5qhv1XzJmkJbU1NjjvWWqfZac+fOnUuNvfrqq+bYvXv3mvFKpqqj9v3G8m78XgCjDf6z7akTRcQr6IiCYLETBcFiJwqCxU4UBIudKAgWO1EQbp+9qHdWwX32StbY2GjGrWmqXq+6urrajHvXH3is5ZwHBwfNsSdPnjTj7777rhm/fPmyGR+v0vrsPLMTBcFiJwqCxU4UBIudKAgWO1EQLHaiIFjsREGUu89+FsCnI26aDSB90nG+KjW3Ss0LYG6FKmZuC1W1drRAWYv9a3cu0l6pa9NVam6VmhfA3ApVrtz4NJ4oCBY7URB5F3tbzvdvqdTcKjUvgLkVqiy55fqanYjKJ+8zOxGVCYudKIhcil1EnhCRQyJyREQ25ZFDGhE5ISL7RWRf3vvTJXvo9YnIgRG31YjILhE5nHy0F24vb24vi8jp5NjtE5G1OeW2QET+KCIHReQjEflRcnuux87IqyzHreyv2UVkIoBPAPwNgC4A7wPYoKoflzWRFCJyAsAKVc39AgwReQTAZQC/UdW/TG77FwD9qro5+Y9ypqr+Y4Xk9jKAy3lv453sVtQwcptxAE8B+DvkeOyMvP4WZThueZzZVwI4oqrHVPU6gN8BWJdDHhVPVfcA6L/t5nUAtiafb8Xwg6XsUnKrCKrao6ofJp8PALi1zXiux87IqyzyKPZ5AE6N+LoLlbXfuwLYKSIfiEhr3smMou7WNlvJxzk553M7dxvvcrptm/GKOXaFbH+eVR7FPtr6WJXU/1ulqssBrAHwg+TpKo3NmLbxLpdRthmvCIVuf55VHsXeBWDBiK/nA+jOIY9RqWp38rEPwJuovK2oe2/toJt87Ms5ny9V0jbeo20zjgo4dnluf55Hsb8PYImINIlIFYDvAdieQx5fIyJTkzdOICJTAXwHlbcV9XYAzySfPwPgrRxz+YpK2cY7bZtx5Hzsct/+XFXL/g/AWgy/I38UwD/lkUNKXncD+L/k30d55wbgdQw/rbuB4WdEzwOYBWA3gMPJx5oKyu01DG/t3YHhwmrIKbfVGH5p2AFgX/Jvbd7HzsirLMeNl8sSBcEr6IiCYLETBcFiJwqCxU4UBIudKAgWO1EQLHaiIP4f/M9nRHh3pTcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [2, 4, 6, 8, 10]:\n",
    "    plt.imshow(array[i], cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n",
      "(10000, 785)\n"
     ]
    }
   ],
   "source": [
    "print(fas_train.shape)\n",
    "print(fas_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6000\n",
       "1    6000\n",
       "2    6000\n",
       "3    6000\n",
       "4    6000\n",
       "5    6000\n",
       "6    6000\n",
       "7    6000\n",
       "8    6000\n",
       "9    6000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fas_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 T-shirt/top  \n",
    "1 Trouser  \n",
    "2 Pullover  \n",
    "3 Dress  \n",
    "4 Coat  \n",
    "5 Sandal  \n",
    "6 Shirt  \n",
    "7 Sneaker  \n",
    "8 Bag  \n",
    "9 Ankle boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "\n",
    "train데이터는 클래스별로 6000개씩 데이터가 있는데 60000개의 데이터로 돌리기에는  \n",
    "시간이 너무 많이 소요(아래에서 실험)  \n",
    "\n",
    "=> 프로젝트임을 고려하여 많은 시간과 정확도보다는 많은 테스트, 성능 향상 등에 초점  \n",
    "\n",
    "=> 각 클래스별로 랜덤하게 1000개씩을 뽑아 10000개의 데이터로 활용하고자 함  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비어있는 DataFrame 생성\n",
    "fas_train_random = pd.DataFrame()\n",
    "\n",
    "# 각 클래스에서 1000개를 랜덤하게 뽑아서 fas_train_random에 저장\n",
    "for i in range(10) :\n",
    "    fas_ = fas_train[fas_train['label'] == i].sample(n=1000)\n",
    "    fas_train_random = fas_train_random.append(fas_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 785)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fas_train_random.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27629</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57308</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>123</td>\n",
       "      <td>119</td>\n",
       "      <td>131</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49291</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>127</td>\n",
       "      <td>68</td>\n",
       "      <td>181</td>\n",
       "      <td>215</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40433</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>133</td>\n",
       "      <td>108</td>\n",
       "      <td>98</td>\n",
       "      <td>113</td>\n",
       "      <td>143</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>139</td>\n",
       "      <td>126</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40932</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>129</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38714</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>111</td>\n",
       "      <td>93</td>\n",
       "      <td>87</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38899</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>93</td>\n",
       "      <td>96</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13547</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12028</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21053</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>107</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7563</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>172</td>\n",
       "      <td>165</td>\n",
       "      <td>156</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42129</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>65</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27652</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>168</td>\n",
       "      <td>163</td>\n",
       "      <td>179</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59769</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>158</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40041</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53458</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17691</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>146</td>\n",
       "      <td>141</td>\n",
       "      <td>174</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44025</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>111</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "27629      0       0       0       0       0       0       0       0       0   \n",
       "57308      0       0       0       0       0       0       0       0       0   \n",
       "49291      0       0       0       0       0       0       0       0       0   \n",
       "40433      0       0       0       0       7     133     108      98     113   \n",
       "40932      0       0       0       0       0       0       0       1       0   \n",
       "38714      0       0       0       0       0       0       0       0       0   \n",
       "38899      0       0       0       0       0       0       1       0       0   \n",
       "4042       0       0       0       0       0       0       0       0       0   \n",
       "13547      0       0       0       0       0       0       2       0       0   \n",
       "12028      0       0       0       0       0       0       0       1       3   \n",
       "21053      0       0       0       0       0       0       0       0       0   \n",
       "7563       0       0       0       0       0       0       0       0       0   \n",
       "42129      0       0       0       0       0       0       0       0       0   \n",
       "7171       0       0       0       0       0       0       0       0       0   \n",
       "27652      0       0       0       2       0       0       0       0       0   \n",
       "59769      0       0       0       0       0       0       0       0       0   \n",
       "40041      0       0       0       0       0       0       0       1       2   \n",
       "53458      0       0       0       0       0       1       1       0       0   \n",
       "17691      0       0       0       0       0       0       0       0       0   \n",
       "44025      0       0       0       0       0       0       0       3       0   \n",
       "\n",
       "       pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "27629       0  ...         0         0         0         0         0   \n",
       "57308       0  ...       123       119       131        38         0   \n",
       "49291       0  ...        62       127        68       181       215   \n",
       "40433     143  ...         0         0         0         4         0   \n",
       "40932       3  ...       144       129        73         0         0   \n",
       "38714       0  ...       111        93        87        15         0   \n",
       "38899       0  ...        93        96        83         0         0   \n",
       "4042        0  ...         0         0         0         0         0   \n",
       "13547       0  ...        19         0         0         0         1   \n",
       "12028       3  ...         0         0         0         1         3   \n",
       "21053       0  ...        99       107        28         0         1   \n",
       "7563        0  ...       172       165       156        36         0   \n",
       "42129       0  ...        81        65         6         0         0   \n",
       "7171        0  ...         0         0         0         0         0   \n",
       "27652      32  ...       168       163       179        19         0   \n",
       "59769       0  ...        96       100       158        17         0   \n",
       "40041       0  ...        69        12         0         0         1   \n",
       "53458       0  ...        41         9         0         0         0   \n",
       "17691       0  ...       146       141       174        26         0   \n",
       "44025       0  ...       111        49         0         0         0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "27629         0         0         2         0         0  \n",
       "57308         0         0         0         0         0  \n",
       "49291       122         0         0         0         0  \n",
       "40433        46       139       126        59         0  \n",
       "40932         2         0         0         0         0  \n",
       "38714         0         0         0         0         0  \n",
       "38899         0         0         0         0         0  \n",
       "4042          0         0         0         0         0  \n",
       "13547         0         0         0         0         0  \n",
       "12028         0         0         0         0         0  \n",
       "21053         0         0         0         0         0  \n",
       "7563          5         0         0         0         0  \n",
       "42129         1         0         0         0         0  \n",
       "7171          0         0         0         0         0  \n",
       "27652         2         0         0         0         0  \n",
       "59769         4         0         0         0         0  \n",
       "40041         0         0         0         0         0  \n",
       "53458         0         0         0         0         0  \n",
       "17691         4         0         0         0         0  \n",
       "44025         0         0         0         0         0  \n",
       "\n",
       "[20 rows x 785 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fas_train_random.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 클래스가 0부터 9까지 정렬되어있는 상태이므로 이를 적절하게 섞어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fas_train_new = fas_train_random.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19828</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>194</td>\n",
       "      <td>170</td>\n",
       "      <td>108</td>\n",
       "      <td>69</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42162</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>154</td>\n",
       "      <td>156</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51591</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26579</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19087</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>69</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52806</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14354</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>163</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57014</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41202</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>110</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33574</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52338</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>227</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8130</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38348</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28163</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45102</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15111</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43098</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22515</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58634</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "19828      1       0       0       0       0       0       0       1       0   \n",
       "42162      0       0       0       0       0       0       0       0       0   \n",
       "51591      2       0       0       0       0       0       0       0       0   \n",
       "26579      6       0       0       0       0       0       0       0       0   \n",
       "19087      6       0       0       0       0       0       0       0       0   \n",
       "52806      3       0       0       0       0       0       0       0       0   \n",
       "14354      3       0       0       0       0       0       0       0       0   \n",
       "5562       8       0       0       0       0       0       0       0       0   \n",
       "57014      7       0       0       0       0       0       0       0       0   \n",
       "41202      6       0       0       0       0       0       0       0       0   \n",
       "33574      4       0       0       0       0       0       0       0       0   \n",
       "52338      2       0       0       0       0       0       0       0       0   \n",
       "8130       1       0       0       0       0       0       0       0       0   \n",
       "38348      0       0       0       0       2       0       2       0       0   \n",
       "28163      9       0       0       0       0       0       0       0       0   \n",
       "45102      1       0       0       0       0       0       0       0       0   \n",
       "15111      8       0       0       0       0       0       0       0       0   \n",
       "43098      1       0       0       0       0       0       0       0       0   \n",
       "22515      5       0       0       0       0       0       0       0       0   \n",
       "58634      5       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "19828       0  ...       194       170       108        69         6   \n",
       "42162       0  ...       154       156       160         0         0   \n",
       "51591       0  ...         2         0         0         0         1   \n",
       "26579       1  ...         0         0         0         0         0   \n",
       "19087       4  ...         0         0         0       151        69   \n",
       "52806       0  ...        73         0         0         0         0   \n",
       "14354       1  ...       163        57         0         0         1   \n",
       "5562        0  ...         0         0         0         0         0   \n",
       "57014       0  ...         0         0         0         0         0   \n",
       "41202       0  ...         0         0         0         0       173   \n",
       "33574       0  ...         0         0         0         0         0   \n",
       "52338       1  ...         0         0       104       227        98   \n",
       "8130        0  ...       126         0         0         0         0   \n",
       "38348       0  ...        80        20         0         0         0   \n",
       "28163       0  ...         0         0         0         0         0   \n",
       "45102       0  ...       144        29         0         0         0   \n",
       "15111       0  ...         0         0         0         0         0   \n",
       "43098       0  ...         0         0         0         0         0   \n",
       "22515       0  ...         0         0         0         0         0   \n",
       "58634       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "19828         0         0         0         0         0  \n",
       "42162         1         0         0         0         0  \n",
       "51591         0         0         0         0         0  \n",
       "26579         0         0         0         0         0  \n",
       "19087         3         0         0         0         0  \n",
       "52806         0         0         0         0         0  \n",
       "14354         0         0         0         0         0  \n",
       "5562          0         0         0         0         0  \n",
       "57014         0         0         0         0         0  \n",
       "41202       110        61         0         0         0  \n",
       "33574         0         0         0         0         0  \n",
       "52338         0         0         0         0         0  \n",
       "8130          0         0         0         0         0  \n",
       "38348         0         3         0         0         0  \n",
       "28163         0         0         0         0         0  \n",
       "45102         0         0         0         0         0  \n",
       "15111         0         0         0         0         0  \n",
       "43098         0         0         0         0         0  \n",
       "22515         0         0         0         0         0  \n",
       "58634         0         0         0         0         0  \n",
       "\n",
       "[20 rows x 785 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fas_train_new.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 원본 데이터(60000) KNN 검증\n",
    "\n",
    "시간 측정을 위해 원본 데이터(60000행)로 knn과 cross_Validation을 사용한 결과  \n",
    "\n",
    "약 30분 이상의 시간이 소요됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(fas_train.drop(['label'], axis = 1), fas_train['label'], test_size = 0.2, random_state = 0)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "cvs = cross_val_score(clf, X = X_val, y = y_val, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8275     0.81791667 0.81416667 0.82458333 0.81583333]\n"
     ]
    }
   ],
   "source": [
    "print(cvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fas_train_new 데이터의 80%를 훈련 데이터로, 20%를 검증 데이터로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fas_train_new.drop(['label'], axis = 1)\n",
    "y = fas_train_new['label']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검증 데이터에 대해서 전체 데이터를 사용했을 때보다 조금 떨어진 성능을 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN을 통한 분류(검증 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79   0.7575 0.775  0.77   0.8025]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_val)\n",
    "\n",
    "cvs = cross_val_score(knn, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[167   0   7   4   1   0  24   0   1   0]\n",
      " [  2 201   2   1   0   0   2   0   0   1]\n",
      " [  9   0 145   2  28   0  14   0   0   0]\n",
      " [ 11   0   3 151   8   0   6   0   1   0]\n",
      " [  1   0  28   7 150   0  27   0   0   0]\n",
      " [  0   0   0   0   0 158   0  39   0  14]\n",
      " [ 44   0  33   4  12   0 111   0   4   0]\n",
      " [  0   0   0   0   0   2   0 168   0  17]\n",
      " [  4   1   4   1   1   0   2   1 182   0]\n",
      " [  0   0   1   0   0   0   0   4   0 189]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.82      0.76       204\n",
      "           1       1.00      0.96      0.98       209\n",
      "           2       0.65      0.73      0.69       198\n",
      "           3       0.89      0.84      0.86       180\n",
      "           4       0.75      0.70      0.73       213\n",
      "           5       0.99      0.75      0.85       211\n",
      "           6       0.60      0.53      0.56       208\n",
      "           7       0.79      0.90      0.84       187\n",
      "           8       0.97      0.93      0.95       196\n",
      "           9       0.86      0.97      0.91       194\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.82      0.81      0.81      2000\n",
      "weighted avg       0.82      0.81      0.81      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN을 통한 분류(테스트 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 세트 준비\n",
    "X_test = fas_test.drop(['label'], axis = 1)\n",
    "y_test = fas_test['label']\n",
    "\n",
    "y_test_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.814  0.8215 0.8165 0.804  0.833 ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(knn, X = X_test, y = y_test, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 : \n",
      "[[821   1  30  17   4   0 114   1  12   0]\n",
      " [ 12 956   6  16   2   0   8   0   0   0]\n",
      " [ 28   2 770  12 106   0  78   1   3   0]\n",
      " [ 64  17  12 861  30   0  16   0   0   0]\n",
      " [  7   0 164  50 697   0  81   0   1   0]\n",
      " [  5   0   2   1   0 730  10 153   5  94]\n",
      " [222   1 136  23  67   0 544   0   7   0]\n",
      " [  0   0   0   0   0   9   0 907   0  84]\n",
      " [  9   2  24   4   9   0  17  11 921   3]\n",
      " [  0   0   1   0   0   9   0  39   0 951]]\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 데이터 : \")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.82      0.76      1000\n",
      "           1       0.98      0.96      0.97      1000\n",
      "           2       0.67      0.77      0.72      1000\n",
      "           3       0.88      0.86      0.87      1000\n",
      "           4       0.76      0.70      0.73      1000\n",
      "           5       0.98      0.73      0.84      1000\n",
      "           6       0.63      0.54      0.58      1000\n",
      "           7       0.82      0.91      0.86      1000\n",
      "           8       0.97      0.92      0.95      1000\n",
      "           9       0.84      0.95      0.89      1000\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 데이터 : \")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN 결과 분석\n",
    "\n",
    "1. 검증 데이터에 비해서 테스트 데이터가 조금 더 정확한 결과를 보임  \n",
    "2. 4번(코트)과 6번(셔츠)가 비교적 낮은 정확도를 보임  \n",
    "3. 비교적 안정적인 결과를 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomForest를 통한 분류(검증 데이터)\n",
    "\n",
    "1. n_estimators = 10, max_depth = 5  \n",
    "- 시간을 계산하기 위해서 작은 값으로 설정(1초 이내)  \n",
    "- 정확도가 매우 낮음(6번의 경우 f1점수가 : 0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.765  0.7525 0.695  0.7325 0.775 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 10, max_depth = 5, random_state = 0)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_val)\n",
    "\n",
    "cvs = cross_val_score(rf, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[166   1  11  20   3   1   1   0   1   0]\n",
      " [  0 181   0  28   0   0   0   0   0   0]\n",
      " [  3   0 133   2  55   0   3   0   2   0]\n",
      " [ 10   1   6 158   4   0   1   0   0   0]\n",
      " [  2   0  28  24 155   0   1   0   3   0]\n",
      " [  0   0   0   0   0 181   0  20   2   8]\n",
      " [ 52   0  52  16  62   1  14   0  11   0]\n",
      " [  0   0   0   0   0   6   0 155   0  26]\n",
      " [  0   0   9   5   4   1   0   0 177   0]\n",
      " [  0   0   0   1   0   4   0  12   0 177]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.81      0.76       204\n",
      "           1       0.99      0.87      0.92       209\n",
      "           2       0.56      0.67      0.61       198\n",
      "           3       0.62      0.88      0.73       180\n",
      "           4       0.55      0.73      0.62       213\n",
      "           5       0.93      0.86      0.89       211\n",
      "           6       0.70      0.07      0.12       208\n",
      "           7       0.83      0.83      0.83       187\n",
      "           8       0.90      0.90      0.90       196\n",
      "           9       0.84      0.91      0.87       194\n",
      "\n",
      "    accuracy                           0.75      2000\n",
      "   macro avg       0.76      0.75      0.73      2000\n",
      "weighted avg       0.76      0.75      0.72      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. n_estimators = 100, max_depth = 20  \n",
    "- 시간 : 약 40초 내외  \n",
    "- 성능은 향상(6번에 대한 정확도가 비약적으로 상승, 다른 부분들도 준수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8225 0.805  0.81   0.8175 0.795 ]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, max_depth = 20, random_state = 0)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_val)\n",
    "\n",
    "cvs = cross_val_score(rf, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[170   0   5   7   1   0  21   0   0   0]\n",
      " [  1 203   1   3   0   0   1   0   0   0]\n",
      " [  2   0 151   3  31   0  10   0   1   0]\n",
      " [  5   0   1 161   8   0   4   0   1   0]\n",
      " [  1   0  17   8 173   0  12   0   2   0]\n",
      " [  0   0   0   0   0 195   0  11   2   3]\n",
      " [ 34   0  23   6  23   0 115   0   7   0]\n",
      " [  0   0   0   0   0   6   0 165   0  16]\n",
      " [  1   0   2   1   2   1   2   0 187   0]\n",
      " [  0   0   0   0   0   4   0   8   0 182]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       204\n",
      "           1       1.00      0.97      0.99       209\n",
      "           2       0.76      0.76      0.76       198\n",
      "           3       0.85      0.89      0.87       180\n",
      "           4       0.73      0.81      0.77       213\n",
      "           5       0.95      0.92      0.94       211\n",
      "           6       0.70      0.55      0.62       208\n",
      "           7       0.90      0.88      0.89       187\n",
      "           8       0.94      0.95      0.94       196\n",
      "           9       0.91      0.94      0.92       194\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. GridSearch를 통해서 최적의 파라미터 추출\n",
    "- 시간 : 2시간 이상\n",
    "- 성능이 2번에 비해서 오히려 안좋음 => 조건을 많이 주는 것이 오히려 안좋을 수 있다고 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth':[30, 50, 70],\n",
    "          'min_samples_leaf':[10, 15, 20],\n",
    "          'min_samples_split':[8, 16, 20],\n",
    "          'n_estimators':[100, 200, 400, 600]\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=RandomForestClassifier(random_state=0),\n",
       "             param_grid={'max_depth': [30, 50, 70],\n",
       "                         'min_samples_leaf': [10, 15, 20],\n",
       "                         'min_samples_split': [8, 16, 20],\n",
       "                         'n_estimators': [100, 200, 400, 600]})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "grid_cv = GridSearchCV(rf, params, cv=2)\n",
    "grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 30,\n",
       " 'min_samples_leaf': 10,\n",
       " 'min_samples_split': 8,\n",
       " 'n_estimators': 600}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8225 0.7875 0.785  0.81   0.79  ]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=600, max_depth=30, min_samples_leaf=10, min_samples_split=8, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_val)\n",
    "\n",
    "cvs = cross_val_score(rf, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[165   1   4   9   0   0  24   0   1   0]\n",
      " [  1 201   3   3   0   0   1   0   0   0]\n",
      " [  4   0 146   1  39   1   5   0   2   0]\n",
      " [  5   1   1 160   7   0   6   0   0   0]\n",
      " [  1   0  14  10 173   0  14   0   1   0]\n",
      " [  0   0   0   0   0 193   0  13   1   4]\n",
      " [ 37   0  27   8  21   0 108   0   7   0]\n",
      " [  0   0   0   0   0   4   0 163   0  20]\n",
      " [  0   0   2   3   2   1   2   0 186   0]\n",
      " [  0   0   0   0   0   3   0   6   0 185]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79       204\n",
      "           1       0.99      0.96      0.98       209\n",
      "           2       0.74      0.74      0.74       198\n",
      "           3       0.82      0.89      0.86       180\n",
      "           4       0.71      0.81      0.76       213\n",
      "           5       0.96      0.91      0.93       211\n",
      "           6       0.68      0.52      0.59       208\n",
      "           7       0.90      0.87      0.88       187\n",
      "           8       0.94      0.95      0.94       196\n",
      "           9       0.89      0.95      0.92       194\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. n_estimators = 500, max_depth = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8325 0.8025 0.8025 0.8225 0.7975]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, max_depth=20, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_val)\n",
    "\n",
    "cvs = cross_val_score(rf, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[169   1   5   7   1   0  20   0   1   0]\n",
      " [  1 203   1   3   0   0   1   0   0   0]\n",
      " [  2   0 151   3  33   0   7   0   2   0]\n",
      " [  5   0   0 162   6   0   7   0   0   0]\n",
      " [  1   0  16   9 173   0  13   0   1   0]\n",
      " [  0   0   0   0   0 196   0  10   2   3]\n",
      " [ 34   0  25   8  20   0 114   0   7   0]\n",
      " [  0   0   0   0   0   3   0 167   0  17]\n",
      " [  1   0   2   1   2   0   1   0 189   0]\n",
      " [  0   0   0   0   0   4   0   6   0 184]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       204\n",
      "           1       1.00      0.97      0.98       209\n",
      "           2       0.76      0.76      0.76       198\n",
      "           3       0.84      0.90      0.87       180\n",
      "           4       0.74      0.81      0.77       213\n",
      "           5       0.97      0.93      0.95       211\n",
      "           6       0.70      0.55      0.61       208\n",
      "           7       0.91      0.89      0.90       187\n",
      "           8       0.94      0.96      0.95       196\n",
      "           9       0.90      0.95      0.92       194\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.86      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomForest를 통한 분류(테스트 데이터)\n",
    "\n",
    "테스트용 데이터 검증 모델로 n_estimators = 500, max_depth = 20 옵션을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.859  0.863  0.8615 0.8535 0.8605]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(rf, X = X_test, y = y_test, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 : \n",
      "[[807   1  18  60   2   2  95   0  15   0]\n",
      " [  2 964   6  21   1   1   5   0   0   0]\n",
      " [  8   3 777   9 135   1  54   0  13   0]\n",
      " [ 22  10   6 920  22   0  19   0   1   0]\n",
      " [  1   1  69  41 848   0  36   0   4   0]\n",
      " [  0   0   0   0   0 923   0  50   5  22]\n",
      " [177   4 118  34  90   0 555   0  22   0]\n",
      " [  0   0   0   0   0  31   0 902   1  66]\n",
      " [  1   1   9   2   4   3   9   4 966   1]\n",
      " [  0   0   1   0   0  12   0  48   3 936]]\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 데이터 : \")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80      1000\n",
      "           1       0.98      0.96      0.97      1000\n",
      "           2       0.77      0.78      0.78      1000\n",
      "           3       0.85      0.92      0.88      1000\n",
      "           4       0.77      0.85      0.81      1000\n",
      "           5       0.95      0.92      0.94      1000\n",
      "           6       0.72      0.56      0.63      1000\n",
      "           7       0.90      0.90      0.90      1000\n",
      "           8       0.94      0.97      0.95      1000\n",
      "           9       0.91      0.94      0.92      1000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 데이터 : \")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomForest 결과 분석\n",
    "\n",
    "1. 검증 데이터에 비해서 테스트 데이터가 조금 더 정확한 결과를 보임  \n",
    "2. KNN에 비하면 전체적으로 높은 정확도  \n",
    "3. 6번은 정확도가 아직 많이 부족하지만 다른 클래스들은 비교적 잘 분류함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost을 통한 분류(검증 데이터)\n",
    "\n",
    "1. default  \n",
    "- 전체적인 정확도는 랜덤 포레스트와 비슷함\n",
    "- 6번에 대한 정확도가 많이 올라감 => XGBoost을 통해서 전체적인 정확도를 보장할 수 있다고 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:50:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:52:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:52:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:53:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:53:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:53:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.865  0.795  0.805  0.815  0.8075]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_val)\n",
    "\n",
    "cvs = cross_val_score(xgb, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[173   0   4   9   1   0  17   0   0   0]\n",
      " [  1 204   0   2   0   0   2   0   0   0]\n",
      " [  7   0 159   1  21   0   9   0   1   0]\n",
      " [  5   0   3 161   8   0   3   0   0   0]\n",
      " [  0   0  15   6 180   0  12   0   0   0]\n",
      " [  0   0   0   0   0 197   0  11   0   3]\n",
      " [ 24   0  26   6  14   0 133   0   5   0]\n",
      " [  0   0   0   0   0   1   0 175   0  11]\n",
      " [  1   0   1   1   4   0   4   0 185   0]\n",
      " [  0   0   0   0   0   2   0   5   0 187]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83       204\n",
      "           1       1.00      0.98      0.99       209\n",
      "           2       0.76      0.80      0.78       198\n",
      "           3       0.87      0.89      0.88       180\n",
      "           4       0.79      0.85      0.82       213\n",
      "           5       0.98      0.93      0.96       211\n",
      "           6       0.74      0.64      0.69       208\n",
      "           7       0.92      0.94      0.93       187\n",
      "           8       0.97      0.94      0.96       196\n",
      "           9       0.93      0.96      0.95       194\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 초기값 설정 (참고 : https://www.kaggle.com/lifesailor/xgboost)\n",
    "- 평균적으로 0.88의 정확도를 보임\n",
    "- 좋은 모델이지만 시간이 많이 소요(cross_validation 과정에서 한 번당 약 20분의 시간이 필요)\n",
    "- n_estimators를 조금 줄이고, 다른 파라미터를 조절하고자 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:06:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:06:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:25:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:25:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:28:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:28:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:31:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:31:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:33:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:33:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:37:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[03:37:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.8575 0.8175 0.815  0.8225 0.835 ]\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=-1,\n",
    "    scale_pos_weight=1,\n",
    "    )\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_val)\n",
    "\n",
    "cvs = cross_val_score(xgb, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[173   1   3   9   0   0  18   0   0   0]\n",
      " [  1 205   0   1   0   0   2   0   0   0]\n",
      " [  5   0 158   2  23   0   9   0   1   0]\n",
      " [  6   1   1 160   7   0   5   0   0   0]\n",
      " [  0   0  19   7 173   0  14   0   0   0]\n",
      " [  0   0   0   0   0 195   0  12   1   3]\n",
      " [ 20   0  22   7  16   0 139   0   4   0]\n",
      " [  0   0   0   0   0   1   0 175   0  11]\n",
      " [  1   0   2   0   3   0   3   1 186   0]\n",
      " [  0   0   0   0   0   2   0   5   0 187]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84       204\n",
      "           1       0.99      0.98      0.99       209\n",
      "           2       0.77      0.80      0.78       198\n",
      "           3       0.86      0.89      0.87       180\n",
      "           4       0.78      0.81      0.80       213\n",
      "           5       0.98      0.92      0.95       211\n",
      "           6       0.73      0.67      0.70       208\n",
      "           7       0.91      0.94      0.92       187\n",
      "           8       0.97      0.95      0.96       196\n",
      "           9       0.93      0.96      0.95       194\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 파라미터 튜닝\n",
    "- 2번보다 정확도가 떨어짐 다른 방향을 고려해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:50:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:58:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:00:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:02:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:04:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:06:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.8575 0.795  0.8175 0.8075 0.8075]\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.01,\n",
    "    n_estimators=400,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=-1,\n",
    "    )\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_val)\n",
    "\n",
    "cvs = cross_val_score(xgb, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[172   1   4   8   0   0  18   0   1   0]\n",
      " [  2 200   2   4   0   0   1   0   0   0]\n",
      " [  4   0 149   2  32   1   9   0   1   0]\n",
      " [  6   1   1 159   8   0   5   0   0   0]\n",
      " [  0   0  18   9 169   0  17   0   0   0]\n",
      " [  0   0   0   0   0 192   0  15   1   3]\n",
      " [ 28   0  24   7  22   1 121   0   5   0]\n",
      " [  0   0   0   0   0   1   0 171   0  15]\n",
      " [  1   0   2   3   5   0   4   0 181   0]\n",
      " [  0   0   0   0   0   1   0   3   0 190]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.82       204\n",
      "           1       0.99      0.96      0.97       209\n",
      "           2       0.74      0.75      0.75       198\n",
      "           3       0.83      0.88      0.85       180\n",
      "           4       0.72      0.79      0.75       213\n",
      "           5       0.98      0.91      0.94       211\n",
      "           6       0.69      0.58      0.63       208\n",
      "           7       0.90      0.91      0.91       187\n",
      "           8       0.96      0.92      0.94       196\n",
      "           9       0.91      0.98      0.95       194\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 파라미터 튜닝\n",
    "- n_estimators를 유지하고, 다른 매개변수를 조절하고자 함\n",
    "- 초기값 모델(1번)에는 조금 못 미치는 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:12:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:22:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:24:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:27:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:29:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:32:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.8625 0.8025 0.8    0.82   0.8225]\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.01,\n",
    "    n_estimators=1000,\n",
    "    max_depth=3,\n",
    "    min_child_weight=5,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=-1,\n",
    "    )\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_val)\n",
    "\n",
    "cvs = cross_val_score(xgb, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[167   1   3  10   0   0  22   0   1   0]\n",
      " [  2 202   1   2   1   0   1   0   0   0]\n",
      " [  3   0 150   1  30   1  12   0   1   0]\n",
      " [  6   1   1 158   9   0   5   0   0   0]\n",
      " [  0   0  20   7 168   0  18   0   0   0]\n",
      " [  0   0   0   0   0 195   0  12   1   3]\n",
      " [ 32   0  22   7  20   0 123   0   4   0]\n",
      " [  0   0   0   0   0   0   0 175   0  12]\n",
      " [  0   0   1   2   4   0   4   0 185   0]\n",
      " [  0   0   0   0   0   1   0   4   0 189]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81       204\n",
      "           1       0.99      0.97      0.98       209\n",
      "           2       0.76      0.76      0.76       198\n",
      "           3       0.84      0.88      0.86       180\n",
      "           4       0.72      0.79      0.76       213\n",
      "           5       0.99      0.92      0.96       211\n",
      "           6       0.66      0.59      0.63       208\n",
      "           7       0.92      0.94      0.93       187\n",
      "           8       0.96      0.94      0.95       196\n",
      "           9       0.93      0.97      0.95       194\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.86      0.86      0.86      2000\n",
      "weighted avg       0.86      0.86      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. GridSearch를 통해서 최적의 파라미터 추출\n",
    "- 시간 : 3시간 30분\n",
    "- 초기값 모델에서 max_depth, min_child_weight, gamma만 변경하여 최적의 파라미터를 찾고자 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:28:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:33:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:39:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:43:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:47:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:51:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:56:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:04:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:10:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:15:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:20:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:24:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:28:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:34:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:41:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:46:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:51:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:00:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:10:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:14:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:19:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:23:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:27:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:33:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:40:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:50:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:55:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:00:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:08:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:16:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:21:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:28:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:32:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:37:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=0.8, gamma=0.1,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=0.1, max_delta_step=None,\n",
       "                                     max_depth=5, min_child_weight=1,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=1000, n_jobs=None, nthread=-1,\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=0.8,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             param_grid={'gamma': [0.1, 0.2], 'max_depth': [3, 4, 5],\n",
       "                         'min_child_weight': [1, 3, 5]})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'max_depth':[3,4,5],\n",
    "          'min_child_weight':[1,3,5],\n",
    "          'gamma':[0.1,0.2]\n",
    "         }\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=-1,\n",
    "    )\n",
    "\n",
    "grid_cv = GridSearchCV(xgb, params, cv=2)\n",
    "grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.1, 'max_depth': 4, 'min_child_weight': 1}\n",
      "0.8612500000000001\n"
     ]
    }
   ],
   "source": [
    "print(grid_cv.best_params_)\n",
    "print(grid_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:41:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=4,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=-1,\n",
    "    )\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:23:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:27:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:31:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:38:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.8575 0.8025 0.805  0.815  0.825 ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(xgb, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[174   1   3   9   0   0  17   0   0   0]\n",
      " [  1 206   0   1   0   0   1   0   0   0]\n",
      " [  7   0 156   2  24   0   7   0   2   0]\n",
      " [  7   1   1 159   8   0   4   0   0   0]\n",
      " [  0   0  17   6 176   0  14   0   0   0]\n",
      " [  0   0   0   0   0 195   0  12   1   3]\n",
      " [ 25   0  22   4  16   0 137   0   4   0]\n",
      " [  0   0   0   0   0   2   0 174   0  11]\n",
      " [  0   0   2   0   3   0   4   0 187   0]\n",
      " [  0   0   0   0   0   1   0   4   0 189]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       204\n",
      "           1       0.99      0.99      0.99       209\n",
      "           2       0.78      0.79      0.78       198\n",
      "           3       0.88      0.88      0.88       180\n",
      "           4       0.78      0.83      0.80       213\n",
      "           5       0.98      0.92      0.95       211\n",
      "           6       0.74      0.66      0.70       208\n",
      "           7       0.92      0.93      0.92       187\n",
      "           8       0.96      0.95      0.96       196\n",
      "           9       0.93      0.97      0.95       194\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost를 통한 분류(테스트 데이터)\n",
    "\n",
    "테스트용 데이터 검증 모델로 GridSearch의 결과로 나온 옵션을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:07:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=4,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=-1,\n",
    "    )\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:26:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:28:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:31:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:33:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:36:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.8575 0.8025 0.805  0.815  0.825 ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(xgb, X = X_test, y = y_test, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 : \n",
      "[[829   0  16  43   1   1  93   2  15   0]\n",
      " [  3 973   5  13   0   2   4   0   0   0]\n",
      " [ 13   2 821  12  77   0  67   0   8   0]\n",
      " [ 20   8   9 918  22   0  23   0   0   0]\n",
      " [  0   0  64  33 845   1  54   0   3   0]\n",
      " [  0   0   0   1   0 934   0  39   4  22]\n",
      " [141   5  92  27  65   1 658   0  11   0]\n",
      " [  0   0   0   0   0  15   0 929   1  55]\n",
      " [  1   1   7   3   3   4  12   2 965   2]\n",
      " [  0   0   0   0   0   5   1  47   4 943]]\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 데이터 : \")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.83      1000\n",
      "           1       0.98      0.97      0.98      1000\n",
      "           2       0.81      0.82      0.82      1000\n",
      "           3       0.87      0.92      0.90      1000\n",
      "           4       0.83      0.84      0.84      1000\n",
      "           5       0.97      0.93      0.95      1000\n",
      "           6       0.72      0.66      0.69      1000\n",
      "           7       0.91      0.93      0.92      1000\n",
      "           8       0.95      0.96      0.96      1000\n",
      "           9       0.92      0.94      0.93      1000\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 데이터 : \")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost 결과 분석\n",
    "\n",
    "1. 검증 데이터에 비해서 테스트 데이터가 조금 더 정확한 결과를 보임  \n",
    "2. 이전의 분류기에 비하면 높은 정확도\n",
    "3. 2번(초기값)과 전체적인 정확도는 비슷하지만, 2번이 특정 값들을 더 잘 분류했다면 최종 결과는 전체적으로 골고루 잘 분류한듯 함\n",
    "4. 6번에 대한 정확도는 0.7 내외가 한계라고 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP를 통한 분류(검증 데이터)\n",
    "\n",
    "1. 초기값 설정  \n",
    "- 데이터의 크기를 고려하여 solver='adam'을 제공하였고, 데이터가 비교적 단순하여 레이어의 사이즈를 2개의 은닉층에 10개의 노드를 사용함.\n",
    "- 모두 3으로 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10),\n",
    "                    alpha=0.0001,\n",
    "                    solver='adam',\n",
    "                    max_iter=100,\n",
    "                    random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1075 0.105  0.1025 0.105  0.105 ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(mlp, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[  0   0   0 204   0   0   0   0   0   0]\n",
      " [  0   0   0 209   0   0   0   0   0   0]\n",
      " [  0   0   0 198   0   0   0   0   0   0]\n",
      " [  0   0   0 180   0   0   0   0   0   0]\n",
      " [  0   0   0 213   0   0   0   0   0   0]\n",
      " [  0   0   0 211   0   0   0   0   0   0]\n",
      " [  0   0   0 208   0   0   0   0   0   0]\n",
      " [  0   0   0 187   0   0   0   0   0   0]\n",
      " [  0   0   0 196   0   0   0   0   0   0]\n",
      " [  0   0   0 194   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       204\n",
      "           1       0.00      0.00      0.00       209\n",
      "           2       0.00      0.00      0.00       198\n",
      "           3       0.09      1.00      0.17       180\n",
      "           4       0.00      0.00      0.00       213\n",
      "           5       0.00      0.00      0.00       211\n",
      "           6       0.00      0.00      0.00       208\n",
      "           7       0.00      0.00      0.00       187\n",
      "           8       0.00      0.00      0.00       196\n",
      "           9       0.00      0.00      0.00       194\n",
      "\n",
      "    accuracy                           0.09      2000\n",
      "   macro avg       0.01      0.10      0.02      2000\n",
      "weighted avg       0.01      0.09      0.01      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 조금 더 복잡한 모델\n",
    "- 이번엔 모두 8로 예측 => 아직은 너무 간단한 모델이라고 생각"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100),\n",
    "                    alpha=0.001,\n",
    "                    solver='sgd',\n",
    "                    max_iter=100,\n",
    "                    random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.105 0.105 0.105 0.105 0.105]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(mlp, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[  0   0   0   0   0   0   0   0 204   0]\n",
      " [  0   0   0   0   0   0   0   0 209   0]\n",
      " [  0   0   0   0   0   0   0   0 198   0]\n",
      " [  0   0   0   0   0   0   0   0 180   0]\n",
      " [  0   0   0   0   0   0   0   0 213   0]\n",
      " [  0   0   0   0   0   0   0   0 211   0]\n",
      " [  0   0   0   0   0   0   0   0 208   0]\n",
      " [  0   0   0   0   0   0   0   0 187   0]\n",
      " [  0   0   0   0   0   0   0   0 196   0]\n",
      " [  0   0   0   0   0   0   0   0 194   0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       204\n",
      "           1       0.00      0.00      0.00       209\n",
      "           2       0.00      0.00      0.00       198\n",
      "           3       0.00      0.00      0.00       180\n",
      "           4       0.00      0.00      0.00       213\n",
      "           5       0.00      0.00      0.00       211\n",
      "           6       0.00      0.00      0.00       208\n",
      "           7       0.00      0.00      0.00       187\n",
      "           8       0.10      1.00      0.18       196\n",
      "           9       0.00      0.00      0.00       194\n",
      "\n",
      "    accuracy                           0.10      2000\n",
      "   macro avg       0.01      0.10      0.02      2000\n",
      "weighted avg       0.01      0.10      0.02      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 파라미터 튜닝\n",
    "- 은닉층을 복잡하게 설계하니 다양한 클래스로 예측함을 확인 => 조금 더 복잡하게 하고자 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 100, 100, 100, 10),\n",
    "                    alpha=0.001,\n",
    "                    solver='sgd',\n",
    "                    max_iter=100,\n",
    "                    random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39   0.2575 0.2    0.1975 0.2125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(mlp, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[ 77   0  71  48   0   0   0   0   7   1]\n",
      " [  0 199   3   6   0   0   0   0   0   1]\n",
      " [ 50   1 119  10   0   0   2   0  16   0]\n",
      " [ 25  40  18  94   0   0   0   1   2   0]\n",
      " [ 90   2  76  38   0   0   1   0   6   0]\n",
      " [  0   0   0   0   0 101   0  58   8  44]\n",
      " [ 54   2 110  24   0   0   1   0  17   0]\n",
      " [  0   0   0   0   0  54   0 118  10   5]\n",
      " [  1   0   6   2   0   0   1   2 184   0]\n",
      " [  0   0   0   0   0  17   0   2   0 175]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.38      0.31       204\n",
      "           1       0.82      0.95      0.88       209\n",
      "           2       0.30      0.60      0.40       198\n",
      "           3       0.42      0.52      0.47       180\n",
      "           4       0.00      0.00      0.00       213\n",
      "           5       0.59      0.48      0.53       211\n",
      "           6       0.20      0.00      0.01       208\n",
      "           7       0.65      0.63      0.64       187\n",
      "           8       0.74      0.94      0.83       196\n",
      "           9       0.77      0.90      0.83       194\n",
      "\n",
      "    accuracy                           0.53      2000\n",
      "   macro avg       0.47      0.54      0.49      2000\n",
      "weighted avg       0.47      0.53      0.48      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 파라미터 튜닝\n",
    "- 더욱 복잡한 은닉층을 만듦\n",
    "- 결과는 아직 좋지 않지만 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 200, 200, 100, 100),\n",
    "                    alpha=0.001,\n",
    "                    solver='sgd',\n",
    "                    max_iter=200,\n",
    "                    random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.795  0.7475 0.78   0.79   0.77  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(mlp, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[162   0   5  10   0   0  27   0   0   0]\n",
      " [  1 204   1   3   0   0   0   0   0   0]\n",
      " [  7   1 143   1  22   0  22   0   2   0]\n",
      " [  9   2   1 153   9   0   4   0   1   1]\n",
      " [  3   0  25   5 160   0  20   0   0   0]\n",
      " [  0   0   0   0   0 184   0  14   4   9]\n",
      " [ 30   0  23   5  18   0 127   0   5   0]\n",
      " [  0   0   0   0   0   9   0 166   0  12]\n",
      " [  0   0   4   3   2   1   3   1 182   0]\n",
      " [  0   0   0   0   0   6   0   8   0 180]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.78       204\n",
      "           1       0.99      0.98      0.98       209\n",
      "           2       0.71      0.72      0.71       198\n",
      "           3       0.85      0.85      0.85       180\n",
      "           4       0.76      0.75      0.75       213\n",
      "           5       0.92      0.87      0.90       211\n",
      "           6       0.63      0.61      0.62       208\n",
      "           7       0.88      0.89      0.88       187\n",
      "           8       0.94      0.93      0.93       196\n",
      "           9       0.89      0.93      0.91       194\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.83      0.83      0.83      2000\n",
      "weighted avg       0.83      0.83      0.83      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 파라미터 튜닝\n",
    "- cross_validation의 결과는 그렇게 좋지는 않지만 꽤나 안정적인 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 200, 200, 300, 200, 200, 100, 100),\n",
    "                    alpha=0.001,\n",
    "                    solver='sgd',\n",
    "                    max_iter=200,\n",
    "                    random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82   0.755  0.7925 0.76   0.7775]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(mlp, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[160   1   6  10   0   1  24   0   2   0]\n",
      " [  1 202   1   3   0   0   2   0   0   0]\n",
      " [  6   0 147   2  25   1  14   0   3   0]\n",
      " [  8   2   2 155   7   0   5   0   1   0]\n",
      " [  1   0  20   9 171   0  11   0   1   0]\n",
      " [  0   0   0   0   0 191   0  14   1   5]\n",
      " [ 22   1  17   7  20   1 134   0   6   0]\n",
      " [  0   0   0   0   0   4   0 170   0  13]\n",
      " [  2   1   3   1   2   1   4   1 180   1]\n",
      " [  0   0   0   0   0   1   0   8   0 185]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79       204\n",
      "           1       0.98      0.97      0.97       209\n",
      "           2       0.75      0.74      0.75       198\n",
      "           3       0.83      0.86      0.84       180\n",
      "           4       0.76      0.80      0.78       213\n",
      "           5       0.95      0.91      0.93       211\n",
      "           6       0.69      0.64      0.67       208\n",
      "           7       0.88      0.91      0.89       187\n",
      "           8       0.93      0.92      0.92       196\n",
      "           9       0.91      0.95      0.93       194\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 파라미터 튜닝\n",
    "- 은닉층의 수를 줄이고 노드 수를 증가시킴\n",
    "- 이전과 비슷한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(200, 200, 200, 300, 200, 200, 200),\n",
    "                    alpha=0.0001,\n",
    "                    solver='sgd',\n",
    "                    max_iter=200,\n",
    "                    random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.815  0.79   0.805  0.77   0.7975]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(mlp, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[159   2   4   8   1   2  28   0   0   0]\n",
      " [  0 204   0   3   1   0   0   0   0   1]\n",
      " [  5   0 149   1  29   0  13   0   1   0]\n",
      " [  8   1   1 157   8   0   4   0   1   0]\n",
      " [  1   1  21   6 170   0  13   0   1   0]\n",
      " [  0   0   0   0   0 183   1  19   1   7]\n",
      " [ 26   0  16   5  18   0 140   0   3   0]\n",
      " [  0   0   0   0   0   6   0 173   0   8]\n",
      " [  1   0   3   3   3   2   5   0 179   0]\n",
      " [  0   0   0   0   0   1   0   8   0 185]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79       204\n",
      "           1       0.98      0.98      0.98       209\n",
      "           2       0.77      0.75      0.76       198\n",
      "           3       0.86      0.87      0.87       180\n",
      "           4       0.74      0.80      0.77       213\n",
      "           5       0.94      0.87      0.90       211\n",
      "           6       0.69      0.67      0.68       208\n",
      "           7       0.86      0.93      0.89       187\n",
      "           8       0.96      0.91      0.94       196\n",
      "           9       0.92      0.95      0.94       194\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 파라미터 튜닝\n",
    "- 단순히 은닉층 수를 늘리고 노드 수를 늘린다고 결과가 좋지는 않음\n",
    "- 적절한 조절이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(300, 300, 300, 300, 300, 300, 300, 300, 300),\n",
    "                    alpha=0.0001,\n",
    "                    solver='sgd',\n",
    "                    max_iter=200,\n",
    "                    random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82   0.7525 0.7975 0.785  0.85  ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(mlp, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[164   0   8  10   0   1  21   0   0   0]\n",
      " [  0 203   2   3   0   0   0   0   0   1]\n",
      " [  6   0 153   2  23   1  13   0   0   0]\n",
      " [  8   1   3 153  10   0   2   0   3   0]\n",
      " [  0   0  26   8 159   0  19   0   1   0]\n",
      " [  0   0   0   0   0 189   0  14   1   7]\n",
      " [ 31   0  16   7  20   0 128   0   6   0]\n",
      " [  0   0   0   0   0   8   0 169   0  10]\n",
      " [  1   0   4   1   1   1   3   0 185   0]\n",
      " [  0   0   0   0   0   3   0   8   0 183]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79       204\n",
      "           1       1.00      0.97      0.98       209\n",
      "           2       0.72      0.77      0.75       198\n",
      "           3       0.83      0.85      0.84       180\n",
      "           4       0.75      0.75      0.75       213\n",
      "           5       0.93      0.90      0.91       211\n",
      "           6       0.69      0.62      0.65       208\n",
      "           7       0.88      0.90      0.89       187\n",
      "           8       0.94      0.94      0.94       196\n",
      "           9       0.91      0.94      0.93       194\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 파라미터 튜닝\n",
    "- 은닉층의 수가 감소 => 한 가지 클래스로 예측\n",
    "- 은닉층은 10개에 가깝도록\n",
    "- 이후에 알파 값을 조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(500, 500, 500),\n",
    "                    alpha=0.01,\n",
    "                    solver='sgd',\n",
    "                    max_iter=200,\n",
    "                    random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\utils\\extmath.py:152: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_base.py:66: RuntimeWarning: invalid value encountered in subtract\n",
      "  tmp = X - X.max(axis=1)[:, np.newaxis]\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45   0.095  0.0925 0.1025 0.0925]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(mlp, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[  0   0   0   0   0   0   0 204   0   0]\n",
      " [  0   0   0   0   0   0   0 209   0   0]\n",
      " [  0   0   0   0   0   0   0 198   0   0]\n",
      " [  0   0   0   0   0   0   0 180   0   0]\n",
      " [  0   0   0   0   0   0   0 213   0   0]\n",
      " [  1   0   0   0   0   0   0 210   0   0]\n",
      " [  0   0   0   0   0   0   0 208   0   0]\n",
      " [  0   0   0   0   0   0   0 187   0   0]\n",
      " [  0   0   0   0   0   0   0 196   0   0]\n",
      " [  0   0   0   0   0   0   0 194   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       204\n",
      "           1       0.00      0.00      0.00       209\n",
      "           2       0.00      0.00      0.00       198\n",
      "           3       0.00      0.00      0.00       180\n",
      "           4       0.00      0.00      0.00       213\n",
      "           5       0.00      0.00      0.00       211\n",
      "           6       0.00      0.00      0.00       208\n",
      "           7       0.09      1.00      0.17       187\n",
      "           8       0.00      0.00      0.00       196\n",
      "           9       0.00      0.00      0.00       194\n",
      "\n",
      "    accuracy                           0.09      2000\n",
      "   macro avg       0.01      0.10      0.02      2000\n",
      "weighted avg       0.01      0.09      0.02      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 파라미터 튜닝\n",
    "- 은닉층의 수를 증가, node를 조절, alpha를 0.01로 변경\n",
    "- 이전보다 뛰어나지는 못한 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 200, 300, 400, 500, 400, 300, 200, 100),\n",
    "                    alpha=0.01,\n",
    "                    solver='sgd',\n",
    "                    max_iter=200,\n",
    "                    random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8075 0.75   0.7525 0.7925 0.8   ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(mlp, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[162   1   7   9   1   0  24   0   0   0]\n",
      " [  3 202   1   2   0   0   1   0   0   0]\n",
      " [  7   0 152   1  26   0  12   0   0   0]\n",
      " [ 11   2   3 151   6   0   6   0   1   0]\n",
      " [  1   1  21   7 166   0  17   0   0   0]\n",
      " [  0   0   0   0   0 193   0  12   1   5]\n",
      " [ 31   0  23   5  27   0 119   0   3   0]\n",
      " [  0   0   0   0   0   6   0 171   0  10]\n",
      " [  2   0   3   1   1   2   4   0 182   1]\n",
      " [  0   0   0   0   0   1   0   6   0 187]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77       204\n",
      "           1       0.98      0.97      0.97       209\n",
      "           2       0.72      0.77      0.75       198\n",
      "           3       0.86      0.84      0.85       180\n",
      "           4       0.73      0.78      0.75       213\n",
      "           5       0.96      0.91      0.93       211\n",
      "           6       0.65      0.57      0.61       208\n",
      "           7       0.90      0.91      0.91       187\n",
      "           8       0.97      0.93      0.95       196\n",
      "           9       0.92      0.96      0.94       194\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 파라미터 튜닝\n",
    "- 이제까지 가장 좋았던 파라미터(6.)을 가져와서 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(200, 200, 200, 200, 200, 200, 200),\n",
    "                    alpha=0.001,\n",
    "                    solver='sgd',\n",
    "                    max_iter=200,\n",
    "                    random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8125 0.7475 0.7825 0.765  0.8   ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(mlp, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[158   1   6  11   1   1  26   0   0   0]\n",
      " [  1 203   1   2   0   0   2   0   0   0]\n",
      " [  6   0 156   1  26   0   9   0   0   0]\n",
      " [  7   3   1 153  11   0   4   0   1   0]\n",
      " [  2   1  25   4 164   0  16   0   1   0]\n",
      " [  0   0   1   0   0 192   0  10   1   7]\n",
      " [ 27   1  25   4  15   0 134   0   2   0]\n",
      " [  0   0   0   0   0   4   0 173   0  10]\n",
      " [  3   0   3   1   2   0   3   0 184   0]\n",
      " [  0   0   0   0   0   5   0  12   0 177]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77       204\n",
      "           1       0.97      0.97      0.97       209\n",
      "           2       0.72      0.79      0.75       198\n",
      "           3       0.87      0.85      0.86       180\n",
      "           4       0.75      0.77      0.76       213\n",
      "           5       0.95      0.91      0.93       211\n",
      "           6       0.69      0.64      0.67       208\n",
      "           7       0.89      0.93      0.91       187\n",
      "           8       0.97      0.94      0.96       196\n",
      "           9       0.91      0.91      0.91       194\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. 파라미터 튜닝\n",
    "- 이전과 비슷한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(300, 300, 300, 300, 300, 300, 300),\n",
    "                    alpha=0.0001,\n",
    "                    solver='sgd',\n",
    "                    max_iter=200,\n",
    "                    random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.815  0.7875 0.7975 0.795  0.81  ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(mlp, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[165   0   3  15   0   1  19   0   1   0]\n",
      " [  0 203   2   2   1   0   1   0   0   0]\n",
      " [  7   1 152   2  23   0  11   0   2   0]\n",
      " [  6   2   3 154  10   1   4   0   0   0]\n",
      " [  2   1  25   6 164   0  13   0   2   0]\n",
      " [  0   0   0   0   0 185   0  14   3   9]\n",
      " [ 27   1  18   6  20   0 130   0   6   0]\n",
      " [  0   0   0   0   0   5   0 174   0   8]\n",
      " [  4   0   4   1   3   3   2   0 179   0]\n",
      " [  0   0   0   0   0   5   0   9   0 180]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.80       204\n",
      "           1       0.98      0.97      0.97       209\n",
      "           2       0.73      0.77      0.75       198\n",
      "           3       0.83      0.86      0.84       180\n",
      "           4       0.74      0.77      0.76       213\n",
      "           5       0.93      0.88      0.90       211\n",
      "           6       0.72      0.62      0.67       208\n",
      "           7       0.88      0.93      0.91       187\n",
      "           8       0.93      0.91      0.92       196\n",
      "           9       0.91      0.93      0.92       194\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP를 통한 분류(테스트 데이터)\n",
    "\n",
    "11. 의 결과를 사용  \n",
    "- 해당 모델 선정 이유 : 다른 것들에 비해서 크게 좋지는 않지만 6번에 대한 예측이 좋고, 전체적으로 안정적이라고 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8445 0.842  0.853  0.838  0.863 ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(mlp, X = X_test, y = y_test, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 : \n",
      "[[796   6  23  42   5   3 111   1  13   0]\n",
      " [  6 964   3  21   1   0   4   0   0   1]\n",
      " [ 25   5 754  17 103   3  84   0   9   0]\n",
      " [ 28  17  15 868  42   0  24   0   6   0]\n",
      " [  2   3  84  37 792   1  73   0   8   0]\n",
      " [  4   0   0   2   0 889   1  60  12  32]\n",
      " [143   5  93  33 101   0 609   0  15   1]\n",
      " [  0   0   0   0   0  39   0 918   0  43]\n",
      " [  8   3   9   2   9   9  12  11 935   2]\n",
      " [  0   0   0   0   0  21   0  48   0 931]]\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 데이터 : \")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.79      1000\n",
      "           1       0.96      0.96      0.96      1000\n",
      "           2       0.77      0.75      0.76      1000\n",
      "           3       0.85      0.87      0.86      1000\n",
      "           4       0.75      0.79      0.77      1000\n",
      "           5       0.92      0.89      0.90      1000\n",
      "           6       0.66      0.61      0.64      1000\n",
      "           7       0.88      0.92      0.90      1000\n",
      "           8       0.94      0.94      0.94      1000\n",
      "           9       0.92      0.93      0.93      1000\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.84      0.85      0.84     10000\n",
      "weighted avg       0.84      0.85      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 데이터 : \")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP 결과 분석\n",
    "\n",
    "1. 파라미터 튜닝에 크게 영향을 받음을 확인 => 은닉층의 수가 적을 때, 한 클래스로만 예측\n",
    "2. 정확도가 XGBoost에 비해서 크게 좋지 않음\n",
    "3. 파라미터 조정이 어려워서 정확도를 높이기 위해서 더 많은 시도가 필요함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 데이터 전처리\n",
    "\n",
    "- 모든 특성의 값이 0~255이므로 256으로 나누어서 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = X.divide(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "255/256 = 0.996이므로 잘 나누어짐을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel1      0.011719\n",
      "pixel2      0.140625\n",
      "pixel3      0.304688\n",
      "pixel4      0.585938\n",
      "pixel5      0.886719\n",
      "              ...   \n",
      "pixel780    0.988281\n",
      "pixel781    0.976562\n",
      "pixel782    0.988281\n",
      "pixel783    0.996094\n",
      "pixel784    0.363281\n",
      "Length: 784, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_scaled.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel1      0.0\n",
      "pixel2      0.0\n",
      "pixel3      0.0\n",
      "pixel4      0.0\n",
      "pixel5      0.0\n",
      "           ... \n",
      "pixel780    0.0\n",
      "pixel781    0.0\n",
      "pixel782    0.0\n",
      "pixel783    0.0\n",
      "pixel784    0.0\n",
      "Length: 784, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_scaled.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel1      0.011719\n",
      "pixel2      0.140625\n",
      "pixel3      0.304688\n",
      "pixel4      0.585938\n",
      "pixel5      0.847656\n",
      "              ...   \n",
      "pixel780    0.988281\n",
      "pixel781    0.976562\n",
      "pixel782    0.988281\n",
      "pixel783    0.996094\n",
      "pixel784    0.257812\n",
      "Length: 784, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### knn(검증데이터, 스케일 조정 후)\n",
    "\n",
    "- 스케일 조정 이전과 결과가 정확하게 일치\n",
    "- 다른 조건이 똑같고 스케일만 조정되었으므로 근접 이웃은 그대로이기 때문이라고 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79   0.7575 0.775  0.77   0.8025]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(knn, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 : \n",
      "[[167   0   7   4   1   0  24   0   1   0]\n",
      " [  2 201   2   1   0   0   2   0   0   1]\n",
      " [  9   0 145   2  28   0  14   0   0   0]\n",
      " [ 11   0   3 151   8   0   6   0   1   0]\n",
      " [  1   0  28   7 150   0  27   0   0   0]\n",
      " [  0   0   0   0   0 158   0  39   0  14]\n",
      " [ 44   0  33   4  12   0 111   0   4   0]\n",
      " [  0   0   0   0   0   2   0 168   0  17]\n",
      " [  4   1   4   1   1   0   2   1 182   0]\n",
      " [  0   0   1   0   0   0   0   4   0 189]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.82      0.76       204\n",
      "           1       1.00      0.96      0.98       209\n",
      "           2       0.65      0.73      0.69       198\n",
      "           3       0.89      0.84      0.86       180\n",
      "           4       0.75      0.70      0.73       213\n",
      "           5       0.99      0.75      0.85       211\n",
      "           6       0.60      0.53      0.56       208\n",
      "           7       0.79      0.90      0.84       187\n",
      "           8       0.97      0.93      0.95       196\n",
      "           9       0.86      0.97      0.91       194\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.82      0.81      0.81      2000\n",
      "weighted avg       0.82      0.81      0.81      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 랜덤포레스트(검증데이터, 스케일 조정 후)\n",
    "\n",
    "- 스케일 조정 이전과 결과가 정확하게 일치  \n",
    "- 강의 내용과 같이 전처리를 하지 않아도 특성 하나하나를 잘 고려하므로 결과가 똑같다고 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, max_depth=20, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8325 0.8025 0.8025 0.8225 0.7975]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(rf, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 : \n",
      "[[169   1   5   7   1   0  20   0   1   0]\n",
      " [  1 203   1   3   0   0   1   0   0   0]\n",
      " [  2   0 151   3  33   0   7   0   2   0]\n",
      " [  5   0   0 162   6   0   7   0   0   0]\n",
      " [  1   0  16   9 173   0  13   0   1   0]\n",
      " [  0   0   0   0   0 196   0  10   2   3]\n",
      " [ 34   0  25   8  20   0 114   0   7   0]\n",
      " [  0   0   0   0   0   3   0 167   0  17]\n",
      " [  1   0   2   1   2   0   1   0 189   0]\n",
      " [  0   0   0   0   0   4   0   6   0 184]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       204\n",
      "           1       1.00      0.97      0.98       209\n",
      "           2       0.76      0.76      0.76       198\n",
      "           3       0.84      0.90      0.87       180\n",
      "           4       0.74      0.81      0.77       213\n",
      "           5       0.97      0.93      0.95       211\n",
      "           6       0.70      0.55      0.61       208\n",
      "           7       0.91      0.89      0.90       187\n",
      "           8       0.94      0.96      0.95       196\n",
      "           9       0.90      0.95      0.92       194\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.86      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP(검증데이터, 스케일 조정 후)\n",
    "\n",
    "- 스케일 조정 이전보다 성능이 향상\n",
    "- 조정이 제대로 되었음을 확인\n",
    "- MLP가 전처리에 영향을 많이 받는다는 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(300, 300, 300, 300, 300, 300, 300),\n",
    "                    alpha=0.0001,\n",
    "                    solver='sgd',\n",
    "                    max_iter=200,\n",
    "                    random_state=0)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 : \n",
      "[[169   0   2  15   0   1  16   0   1   0]\n",
      " [  0 204   0   2   2   0   0   0   0   1]\n",
      " [  2   3 148   4  29   0  12   0   0   0]\n",
      " [  9   4   1 159   7   0   0   0   0   0]\n",
      " [  2   0  18  10 170   0  12   0   1   0]\n",
      " [  0   0   0   0   0 194   0  13   1   3]\n",
      " [ 27   0  18   9  21   0 127   0   6   0]\n",
      " [  0   0   0   0   0   3   0 178   0   6]\n",
      " [  1   1   3   2   3   1   3   0 182   0]\n",
      " [  0   0   0   0   0   3   0   9   0 182]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82       204\n",
      "           1       0.96      0.98      0.97       209\n",
      "           2       0.78      0.75      0.76       198\n",
      "           3       0.79      0.88      0.83       180\n",
      "           4       0.73      0.80      0.76       213\n",
      "           5       0.96      0.92      0.94       211\n",
      "           6       0.75      0.61      0.67       208\n",
      "           7       0.89      0.95      0.92       187\n",
      "           8       0.95      0.93      0.94       196\n",
      "           9       0.95      0.94      0.94       194\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.86      0.86      0.86      2000\n",
      "weighted avg       0.86      0.86      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGB(검증데이터, 스케일 조정 후)\n",
    "\n",
    "- 스케일 조정 이전과 결과가 정확하게 일치  \n",
    "- XGB도 트리기반의 분류기이기 때문  \n",
    "- 스케일 조정 이후에도 XGB가 가장 좋은 성능을 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:48:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=4,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=-1,\n",
    "    )\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[174   1   3   9   0   0  17   0   0   0]\n",
      " [  1 206   0   1   0   0   1   0   0   0]\n",
      " [  7   0 156   2  24   0   7   0   2   0]\n",
      " [  7   1   1 159   8   0   4   0   0   0]\n",
      " [  0   0  17   6 176   0  14   0   0   0]\n",
      " [  0   0   0   0   0 195   0  12   1   3]\n",
      " [ 25   0  22   4  16   0 137   0   4   0]\n",
      " [  0   0   0   0   0   2   0 174   0  11]\n",
      " [  0   0   2   0   3   0   4   0 187   0]\n",
      " [  0   0   0   0   0   1   0   4   0 189]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       204\n",
      "           1       0.99      0.99      0.99       209\n",
      "           2       0.78      0.79      0.78       198\n",
      "           3       0.88      0.88      0.88       180\n",
      "           4       0.78      0.83      0.80       213\n",
      "           5       0.98      0.92      0.95       211\n",
      "           6       0.74      0.66      0.70       208\n",
      "           7       0.92      0.93      0.92       187\n",
      "           8       0.96      0.95      0.96       196\n",
      "           9       0.93      0.97      0.95       194\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VotingClassifier 통한 분류(검증 데이터)\n",
    "\n",
    "1. hard방법  \n",
    "- xgb와 비슷한 성능을 보임  \n",
    "- 비교적 정확한 모델을 사용하여 적용하였기에 높은 성능  \n",
    "- xgb와 mlp가 포함되어 있어서 시간이 많이 소요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "vt = VotingClassifier(estimators=[('rf', rf), ('xgb', xgb), ('mlp', mlp)], voting='hard')\n",
    "vt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = vt.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:51:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:57:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:04:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:10:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:16:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.855  0.8075 0.815  0.8325 0.8275]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(vt, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[175   1   4  10   0   0  13   0   1   0]\n",
      " [  1 206   0   1   0   0   1   0   0   0]\n",
      " [  4   1 154   2  28   0   7   0   2   0]\n",
      " [  6   0   1 163   7   0   3   0   0   0]\n",
      " [  2   0  16  10 176   0   9   0   0   0]\n",
      " [  0   0   0   0   0 197   0  10   1   3]\n",
      " [ 27   0  24   7  19   0 127   0   4   0]\n",
      " [  0   0   0   0   0   2   0 175   0  10]\n",
      " [  0   0   3   1   2   0   2   0 188   0]\n",
      " [  0   0   0   0   0   2   0   4   0 188]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.84       204\n",
      "           1       0.99      0.99      0.99       209\n",
      "           2       0.76      0.78      0.77       198\n",
      "           3       0.84      0.91      0.87       180\n",
      "           4       0.76      0.83      0.79       213\n",
      "           5       0.98      0.93      0.96       211\n",
      "           6       0.78      0.61      0.69       208\n",
      "           7       0.93      0.94      0.93       187\n",
      "           8       0.96      0.96      0.96       196\n",
      "           9       0.94      0.97      0.95       194\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.88      0.88      0.87      2000\n",
      "weighted avg       0.87      0.87      0.87      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VotingClassifier 통한 분류(검증 데이터)\n",
    "\n",
    "2. soft방법    \n",
    "- hard방법 보다 더 높은 성능  \n",
    "- 여태껏 가장 좋은 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:25:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vt = VotingClassifier(estimators=[('rf', rf), ('xgb', xgb), ('mlp', mlp)], voting='soft')\n",
    "vt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = vt.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:50:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:02:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:07:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:12:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:18:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8475 0.805  0.805  0.815  0.8375]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(vt, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[175   0   4   9   0   1  15   0   0   0]\n",
      " [  1 206   0   1   0   0   1   0   0   0]\n",
      " [  3   2 155   4  27   0   6   0   1   0]\n",
      " [  4   0   1 167   7   0   1   0   0   0]\n",
      " [  2   0  16   9 174   0  11   0   1   0]\n",
      " [  0   0   0   0   0 198   0  10   0   3]\n",
      " [ 25   0  21   8  16   0 132   0   6   0]\n",
      " [  0   0   0   0   0   2   0 176   0   9]\n",
      " [  1   0   3   1   2   0   2   0 187   0]\n",
      " [  0   0   0   0   0   2   0   4   0 188]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84       204\n",
      "           1       0.99      0.99      0.99       209\n",
      "           2       0.78      0.78      0.78       198\n",
      "           3       0.84      0.93      0.88       180\n",
      "           4       0.77      0.82      0.79       213\n",
      "           5       0.98      0.94      0.96       211\n",
      "           6       0.79      0.63      0.70       208\n",
      "           7       0.93      0.94      0.93       187\n",
      "           8       0.96      0.95      0.96       196\n",
      "           9       0.94      0.97      0.95       194\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VotingClassifier 통한 분류(검증 데이터)\n",
    "\n",
    "3. soft방법 + weight   \n",
    "- xgb가 정확하였기에 xgb에 가중치를 주고 테스트  \n",
    "- weight를 주기 전에 비해서 오히려 떨어진 f1-score => 정확한 예측을 위해 각 분류기가 중요하게 작용하고 있음을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:39:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vt = VotingClassifier(estimators=[('rf', rf), ('xgb', xgb), ('mlp', mlp)], weights=[1,2,1], voting='soft')\n",
    "vt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = vt.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:28:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:35:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:42:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:50:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:01:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8525 0.8025 0.8075 0.83   0.8375]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(vt, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[175   0   3   9   0   1  16   0   0   0]\n",
      " [  1 206   0   1   0   0   1   0   0   0]\n",
      " [  5   0 160   0  24   0   7   0   2   0]\n",
      " [  4   1   1 164   7   0   3   0   0   0]\n",
      " [  0   0  17   8 174   0  13   0   1   0]\n",
      " [  0   0   0   0   0 196   0  12   0   3]\n",
      " [ 24   0  20   7  16   0 136   0   5   0]\n",
      " [  0   0   0   0   0   2   0 174   0  11]\n",
      " [  1   0   2   1   2   0   3   0 187   0]\n",
      " [  0   0   0   0   0   2   0   3   0 189]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.85       204\n",
      "           1       1.00      0.99      0.99       209\n",
      "           2       0.79      0.81      0.80       198\n",
      "           3       0.86      0.91      0.89       180\n",
      "           4       0.78      0.82      0.80       213\n",
      "           5       0.98      0.93      0.95       211\n",
      "           6       0.76      0.65      0.70       208\n",
      "           7       0.92      0.93      0.93       187\n",
      "           8       0.96      0.95      0.96       196\n",
      "           9       0.93      0.97      0.95       194\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 학습을 위해서 X와 y를 사용  \n",
    "X = X_train + X_val\n",
    "y = y_train + y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VotingClassifier 통한 분류(테스트 데이터)\n",
    "\n",
    "- 가장 높은 성능을 보였던 soft방법을 선택  \n",
    "- 이전에는 훈련 데이터만으로 모델을 학습시켰지만, 훈련과 검증 데이터를 사용하도록 정정  \n",
    "- 크게 좋아지지는 않았지만 역시나 좋은 결과를 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:18:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "vt = VotingClassifier(estimators=[('rf', rf), ('xgb', xgb), ('mlp', mlp)], voting='soft')\n",
    "vt.fit(X, y)\n",
    "\n",
    "y_test_pred = vt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:48:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:16:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:41:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:05:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:00:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.872  0.8665 0.8785 0.867  0.888 ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(vt, X = X_test, y = y_test, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 : \n",
      "[[833   0  17  39   2   0  92   1  16   0]\n",
      " [  2 975   5  14   1   1   2   0   0   0]\n",
      " [ 19   1 808  12  85   0  65   0  10   0]\n",
      " [ 24   8   9 910  30   0  17   0   2   0]\n",
      " [  1   1  65  33 846   0  50   0   4   0]\n",
      " [  0   0   0   0   0 929   0  39   8  24]\n",
      " [145   3  90  18  80   1 649   0  14   0]\n",
      " [  0   0   0   0   0  18   0 943   0  39]\n",
      " [  4   0   6   3   3   6  10   4 961   3]\n",
      " [  0   0   0   0   0  10   0  46   1 943]]\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 데이터 : \")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82      1000\n",
      "           1       0.99      0.97      0.98      1000\n",
      "           2       0.81      0.81      0.81      1000\n",
      "           3       0.88      0.91      0.90      1000\n",
      "           4       0.81      0.85      0.83      1000\n",
      "           5       0.96      0.93      0.95      1000\n",
      "           6       0.73      0.65      0.69      1000\n",
      "           7       0.91      0.94      0.93      1000\n",
      "           8       0.95      0.96      0.95      1000\n",
      "           9       0.93      0.94      0.94      1000\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 데이터 : \")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VotingClassifier 결과 분석\n",
    "\n",
    "1. 기존의 모델들을 사용할 수 있어서 간편함  \n",
    "2. 구성하는 모델들에 따라서 시간이 다르게 소요  \n",
    "3. 안정적인 모델들을 사용하였기에 비교적 정확한 결과를 보임  \n",
    "4. xgb의 성능을 다른 분류기가 떨어뜨릴 수 있다고 예상하였는데, 이 예상이 틀렸다고 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA\n",
    "\n",
    "- 784개의 특성을 모두 고려하는 것은 많은 시간이 소요됨  \n",
    "- 이를 축소하여 비슷한 결과를 내보고자 PCA를 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### explained_variance_ratio\n",
    "\n",
    "- Explained Variance Ratio은 각각의 주성분 벡터가 이루는 축에 투영(projection)한 결과의 분산의 비율  \n",
    "- 주성분이 2개일 때, 가장 수치가 높지만 그 값이 크지 않기 때문에 더 많은 성분을 고려해야함을 확인\n",
    "- 그 특성이 결과를 내는데 얼마나 중요하게 작용하는지를 나타내는 지표라고 생각함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29197364 0.17629911]\n",
      "[0.29197364 0.17629911 0.0598376 ]\n",
      "[0.29197364 0.17629911 0.0598376  0.04930154]\n",
      "[0.29197364 0.17629911 0.0598376  0.04930154 0.03821496]\n",
      "[0.29197364 0.17629911 0.0598376  0.04930154 0.03821496 0.03489686]\n",
      "[0.29197364 0.17629911 0.0598376  0.04930154 0.03821496 0.03489686\n",
      " 0.02346297]\n",
      "[0.29197364 0.17629911 0.0598376  0.04930154 0.03821496 0.03489686\n",
      " 0.02346297 0.01953313]\n",
      "[0.29197364 0.17629911 0.0598376  0.04930154 0.03821496 0.03489686\n",
      " 0.02346297 0.01953313 0.01362034]\n",
      "[0.29197364 0.17629911 0.0598376  0.04930154 0.03821496 0.03489686\n",
      " 0.02346297 0.01953313 0.01362034 0.01301582]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "for n in range(2,11) :\n",
    "    pca = PCA(n_components=n)\n",
    "\n",
    "    printcipalComponents = pca.fit_transform(X_train)\n",
    "\n",
    "    principalDf = pd.DataFrame(data=printcipalComponents)\n",
    "\n",
    "    print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA(n = 2)\n",
    "\n",
    "- pca를 적용시키고 결과를 RandomForest로 분류하고자 하였음  \n",
    "- Pipeline을 사용  \n",
    "- 주성분이 2개일 때는 많이 낮은 결과를 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('pca', PCA(n_components=2)),\n",
    "                 ('rf', rf)])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52   0.5375 0.5175 0.515  0.505 ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(pipe, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[108   2   9  38  18   2  26   0   1   0]\n",
      " [  7 160   4  34   3   0   0   0   1   0]\n",
      " [  6   1  82   2  47   3  44   0  11   2]\n",
      " [ 34  34   4  99   5   0   3   0   1   0]\n",
      " [ 16   0  57  12  83   0  33   0  11   1]\n",
      " [  0   0   2   0   0 112   4  78   9   6]\n",
      " [ 40   1  58  12  32   0  51   0  12   2]\n",
      " [  0   0   0   0   0  30   0 127   1  29]\n",
      " [  3   0  19   2   6   7  12   2 107  38]\n",
      " [  0   0   1   0   1  14   4  18  36 120]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.53      0.52       204\n",
      "           1       0.81      0.77      0.79       209\n",
      "           2       0.35      0.41      0.38       198\n",
      "           3       0.50      0.55      0.52       180\n",
      "           4       0.43      0.39      0.41       213\n",
      "           5       0.67      0.53      0.59       211\n",
      "           6       0.29      0.25      0.26       208\n",
      "           7       0.56      0.68      0.62       187\n",
      "           8       0.56      0.55      0.55       196\n",
      "           9       0.61      0.62      0.61       194\n",
      "\n",
      "    accuracy                           0.52      2000\n",
      "   macro avg       0.53      0.53      0.52      2000\n",
      "weighted avg       0.53      0.52      0.52      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA(n = 10)\n",
    "\n",
    "- 특성을 2개만 사용하는 것은 너무 적다고 판단하여 10개로 확장\n",
    "- 정확도가 좋아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('pca', PCA(n_components=10)),\n",
    "                 ('rf', rf)])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7775 0.755  0.7325 0.785  0.755 ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(pipe, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[158   1   5  12   0   1  26   0   1   0]\n",
      " [  0 198   2   7   1   0   0   0   0   1]\n",
      " [  5   0 131   1  41   0  17   0   3   0]\n",
      " [  6   0   2 153  10   0   5   0   4   0]\n",
      " [  1   1  30   7 150   0  21   0   3   0]\n",
      " [  0   0   0   0   0 184   0  19   1   7]\n",
      " [ 32   0  29   7  16   0 117   0   7   0]\n",
      " [  0   0   0   0   0  15   0 153   1  18]\n",
      " [  0   0   2   2   1   2   4   0 185   0]\n",
      " [  0   0   0   0   0   8   0   8   0 178]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.78       204\n",
      "           1       0.99      0.95      0.97       209\n",
      "           2       0.65      0.66      0.66       198\n",
      "           3       0.81      0.85      0.83       180\n",
      "           4       0.68      0.70      0.69       213\n",
      "           5       0.88      0.87      0.87       211\n",
      "           6       0.62      0.56      0.59       208\n",
      "           7       0.85      0.82      0.83       187\n",
      "           8       0.90      0.94      0.92       196\n",
      "           9       0.87      0.92      0.89       194\n",
      "\n",
      "    accuracy                           0.80      2000\n",
      "   macro avg       0.80      0.81      0.80      2000\n",
      "weighted avg       0.80      0.80      0.80      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA(n = 20)\n",
    "\n",
    "- 특성을 20개 사용  \n",
    "- 784개의 특성을 고려했을 때보다는 낮은 성능을 보이지만 충분히 높은 성능  \n",
    "- 시간이 많이 단축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('pca', PCA(n_components=20)),\n",
    "                 ('rf', rf)])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8025 0.7625 0.7725 0.795  0.795 ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(pipe, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[162   0   4  13   0   1  23   0   1   0]\n",
      " [  2 199   2   5   0   0   0   0   0   1]\n",
      " [  4   0 148   2  30   0  12   0   2   0]\n",
      " [  5   0   2 157   9   0   5   0   2   0]\n",
      " [  1   0  18   8 164   0  21   0   1   0]\n",
      " [  0   0   0   0   0 191   0  10   1   9]\n",
      " [ 33   0  24   5  20   0 121   0   5   0]\n",
      " [  0   0   0   0   0  14   0 154   0  19]\n",
      " [  0   0   2   3   1   2   3   0 185   0]\n",
      " [  0   0   0   0   0   7   0   3   0 184]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.79       204\n",
      "           1       1.00      0.95      0.98       209\n",
      "           2       0.74      0.75      0.74       198\n",
      "           3       0.81      0.87      0.84       180\n",
      "           4       0.73      0.77      0.75       213\n",
      "           5       0.89      0.91      0.90       211\n",
      "           6       0.65      0.58      0.62       208\n",
      "           7       0.92      0.82      0.87       187\n",
      "           8       0.94      0.94      0.94       196\n",
      "           9       0.86      0.95      0.90       194\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.83      0.83      0.83      2000\n",
      "weighted avg       0.83      0.83      0.83      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA(n = 20)\n",
    "\n",
    "- xgb를 사용    \n",
    "- 784개의 특성을 고려했을 때보다는 낮은 성능을 보이지만 충분히 높은 성능  \n",
    "- 시간이 많이 단축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:10:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('pca', PCA(n_components=20)),\n",
    "                 ('xgb', xgb)])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:12:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:13:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:13:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:14:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:14:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.79   0.7625 0.7925 0.805  0.805 ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(pipe, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[163   0   6   6   1   0  27   0   1   0]\n",
      " [  2 202   1   2   1   0   1   0   0   0]\n",
      " [  4   0 149   2  28   0  14   0   1   0]\n",
      " [  5   0   3 157   9   0   5   0   1   0]\n",
      " [  1   0  24   7 161   0  18   0   2   0]\n",
      " [  0   0   0   0   0 193   0  11   1   6]\n",
      " [ 31   0  20   5  18   0 130   0   4   0]\n",
      " [  0   0   0   0   0   9   0 160   0  18]\n",
      " [  1   0   2   2   1   2   3   0 185   0]\n",
      " [  0   0   0   0   0   5   0   6   0 183]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.79       204\n",
      "           1       1.00      0.97      0.98       209\n",
      "           2       0.73      0.75      0.74       198\n",
      "           3       0.87      0.87      0.87       180\n",
      "           4       0.74      0.76      0.75       213\n",
      "           5       0.92      0.91      0.92       211\n",
      "           6       0.66      0.62      0.64       208\n",
      "           7       0.90      0.86      0.88       187\n",
      "           8       0.95      0.94      0.95       196\n",
      "           9       0.88      0.94      0.91       194\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA(n = 20, whiten=True)\n",
    "\n",
    "- whitening의 유무에 따라 성능의 차이를 비교  \n",
    "- 조금씩 차이는 있지만 크게 성능이 달라지지 않았음을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:43:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('pca', PCA(n_components=20, whiten=True)),\n",
    "                 ('xgb', xgb)])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:46:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:46:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:46:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:47:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:47:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.785  0.755  0.79   0.8025 0.81  ]\n"
     ]
    }
   ],
   "source": [
    "cvs = cross_val_score(pipe, X = X_val, y = y_val, cv = 5)\n",
    "print(cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "[[163   0   5   8   2   0  26   0   0   0]\n",
      " [  2 203   0   1   1   0   2   0   0   0]\n",
      " [  3   0 150   2  32   0  10   0   1   0]\n",
      " [  6   0   3 155   9   0   5   0   2   0]\n",
      " [  1   0  24   7 159   0  20   0   2   0]\n",
      " [  0   0   0   0   0 194   0  10   1   6]\n",
      " [ 34   1  19   5  16   0 128   0   5   0]\n",
      " [  0   0   0   0   0  10   0 160   0  17]\n",
      " [  1   0   2   2   1   1   4   0 185   0]\n",
      " [  0   0   0   0   0   5   0   7   0 182]]\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터 : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79       204\n",
      "           1       1.00      0.97      0.98       209\n",
      "           2       0.74      0.76      0.75       198\n",
      "           3       0.86      0.86      0.86       180\n",
      "           4       0.72      0.75      0.73       213\n",
      "           5       0.92      0.92      0.92       211\n",
      "           6       0.66      0.62      0.64       208\n",
      "           7       0.90      0.86      0.88       187\n",
      "           8       0.94      0.94      0.94       196\n",
      "           9       0.89      0.94      0.91       194\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"검증 데이터 : \")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 직접 가져온 이미지를 적용\n",
    "\n",
    "- 인터넷 상에서 가져온 이미지를 분류할 수 있는지 확인하고자 함  \n",
    "- 이미지를 gray컬러로 가져와서 array로 만들고 이를 통해 dataframe으로 만들어 분류기에 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as pilimg\n",
    " \n",
    "# Read image\n",
    "im = pilimg.open('C:/Users/지성이/Desktop/학교/4학년/기계학습/testData/티셔츠1.jpg').convert('L')\n",
    " \n",
    "# Fetch image pixel data to numpy array\n",
    "pix = np.array(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   1   1   2   1   1   4   0   0   1   0   2   0   2   1   0   0   1\n",
      "    4   0   3   1   0   2   0   0   0   0]\n",
      " [  5   1   1   1   1   1   1   0   0   3   0   0   4   0   0   8   5   0\n",
      "    0   0   0   1   6   0   0   0   0   0]\n",
      " [  1   5   1   1   1   1   6   0   1   4   7   0   2   4   0   0   0   0\n",
      "    1   1   8   0   2   0   0   0   0   0]\n",
      " [  3   1   5   1   1   2   0   0   0   0   0  18   0   0   3   2  44   0\n",
      "    1   0   0   6   0   2   0   0   0   0]\n",
      " [  2   1   1   1   9   1   0   5   3  54 224 228 234 218 213 231 229 193\n",
      "   13   6   0   0   7   0   0   0   0   0]\n",
      " [  1   1   3   1   0   1   0   0 172 234 242 235 217 232 237 229 232 240\n",
      "  229 204  26   0   0   0   0   0   0   0]\n",
      " [  6   1   9   0   0   0  49 229 229 249 223 237 237 238 233 231 238 225\n",
      "  235 231 234  16   3   7   0   0   0   0]\n",
      " [  0   8   0   3   1  78 222 230 238 228 241 235 234 234 231 238 235 237\n",
      "  232 238 228 228  40   0   0   0   0   0]\n",
      " [  0   1   0   2 162 230 235 231 235 235 235 235 235 235 235 235 239 234\n",
      "  236 236 234 227 232 166   3   0   1   3]\n",
      " [  1   5   2 126 230 242 235 234 235 235 235 235 235 235 236 236 233 234\n",
      "  240 234 225 241 235 231 143   5   0   5]\n",
      " [  0   0 139 230 245 232 232 236 235 235 235 235 235 235 236 236 241 231\n",
      "  237 240 237 229 227 232 231 156   0   0]\n",
      " [  5   4   0 209 228 233 239 231 235 235 235 235 235 236 236 236 232 239\n",
      "  235 229 235 242 226 233 218   1   2   3]\n",
      " [  0   0   1   3 181 241 230 232 234 235 235 235 235 236 236 236 235 237\n",
      "  239 237 237 223 238 137   0   0   0   1]\n",
      " [  0   2   0   0   0 199 232 222 234 234 235 235 235 236 236 236 238 235\n",
      "  234 230 229 236 195   0   1  10   0   0]\n",
      " [  7   0   2   0   1  13   0 165 234 234 235 235 235 236 236 236 234 234\n",
      "  235 240 153 105   4   0   0   4   0   5]\n",
      " [  0   1   0   4   0   0   3 201 234 234 235 235 235 236 236 236 235 241\n",
      "  235 232 118   0   0   0   2   0   0   0]\n",
      " [  1   1   0   2   0   0   5 169 233 234 235 236 237 236 236 236 236 238\n",
      "  234 231 162   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   2   0   0 163 233 234 235 236 236 236 236 235 235 237\n",
      "  235 233 165   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   5   0   0 174 233 234 235 235 236 236 235 235 234 237\n",
      "  236 235 167   2   0   0   0   0   0   0]\n",
      " [  0   2   0   0   4   0   2 201 233 234 234 235 235 235 235 235 233 236\n",
      "  235 235 168   2   0   0   0   0   0   0]\n",
      " [  0   2   2   0   2   0   5 216 234 234 235 235 235 235 235 235 234 236\n",
      "  234 235 167   1   0   1   0   0   0   0]\n",
      " [  1   0   0   0   2   0   2 210 235 235 235 235 235 235 236 236 236 236\n",
      "  234 235 167   0   0   1   0   0   0   0]\n",
      " [  2   0   0   0   2   0   0 203 236 236 235 235 236 236 236 236 237 237\n",
      "  235 236 168   0   0   1   0   0   0   0]\n",
      " [  1   0   3   0   0   0   1 205 236 236 236 236 236 236 236 237 237 237\n",
      "  236 237 169   1   0   1   0   0   0   0]\n",
      " [  0   0   0   1   1   0   0 204 236 237 239 238 237 236 237 239 235 239\n",
      "  235 237 172   0   4   0   0   0   0   0]\n",
      " [  0   4   0   0   0   0   0  57 171 184 204 221 231 233 231 229 234 227\n",
      "  230 230 158   6   0   3   0   0   0   0]\n",
      " [  0   0   0   2   2   2   5   0   0   0   3   4   2   1   0   0   0   0\n",
      "    1   0   1   0   2   4   0   0   0   0]\n",
      " [  2   0   3   0   0   1   0   2   0   0   0   0   0   0   0   1   2   0\n",
      "    5   0   0  10   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_pix = pix.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = pd.DataFrame(new_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test=new_test.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest에 티셔츠를 넣었는데 6(셔츠)가 결과로 나옴을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, max_depth=20, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "test_pred = rf.predict(new_test)\n",
    "\n",
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 지정\n",
    "category = ['티셔츠', '트라우저', '풀오버', '드레스', '코트' , '샌들', '셔츠', '스니커', '가방', '앵클부츠']\n",
    "\n",
    "# 새로 테스트할 데이터\n",
    "new_test_data = pd.DataFrame()\n",
    "\n",
    "# 모든 파일 읽어오기\n",
    "for c in category :\n",
    "    for n in range(1,6) :\n",
    "        file = 'C:/Users/지성이/Desktop/학교/4학년/기계학습/testData/' + c + str(n) + '.jpg'\n",
    "       \n",
    "        # Read image\n",
    "        im = pilimg.open(str(file)).convert('L')\n",
    " \n",
    "        # Fetch image pixel data to numpy array\n",
    "        pix = np.array(im)\n",
    "        \n",
    "        # (784,)\n",
    "        new_pix = pix.reshape(-1,)\n",
    "\n",
    "        # (784, 1)\n",
    "        new_test = pd.DataFrame(new_pix)\n",
    "        \n",
    "        # (1, 784)\n",
    "        new_test = new_test.transpose()\n",
    "        \n",
    "        # append\n",
    "        new_test_data = new_test_data.append(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 784)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_test = new_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 784)\n"
     ]
    }
   ],
   "source": [
    "print(new_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = []\n",
    "\n",
    "for i in range(0,10) :\n",
    "    for j in range(5) :\n",
    "        answer.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = np.array(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로 테스트할 dataframe의 label값\n",
    "answer = pd.Series(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "print(answer.__class__)\n",
    "print(answer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. xgb사용\n",
    "\n",
    "- 0.5의 정확도를 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "발생한 Error는 특성의 이름이 일치하지 않기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:16:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6', 'pixel7', 'pixel8', 'pixel9', 'pixel10', 'pixel11', 'pixel12', 'pixel13', 'pixel14', 'pixel15', 'pixel16', 'pixel17', 'pixel18', 'pixel19', 'pixel20', 'pixel21', 'pixel22', 'pixel23', 'pixel24', 'pixel25', 'pixel26', 'pixel27', 'pixel28', 'pixel29', 'pixel30', 'pixel31', 'pixel32', 'pixel33', 'pixel34', 'pixel35', 'pixel36', 'pixel37', 'pixel38', 'pixel39', 'pixel40', 'pixel41', 'pixel42', 'pixel43', 'pixel44', 'pixel45', 'pixel46', 'pixel47', 'pixel48', 'pixel49', 'pixel50', 'pixel51', 'pixel52', 'pixel53', 'pixel54', 'pixel55', 'pixel56', 'pixel57', 'pixel58', 'pixel59', 'pixel60', 'pixel61', 'pixel62', 'pixel63', 'pixel64', 'pixel65', 'pixel66', 'pixel67', 'pixel68', 'pixel69', 'pixel70', 'pixel71', 'pixel72', 'pixel73', 'pixel74', 'pixel75', 'pixel76', 'pixel77', 'pixel78', 'pixel79', 'pixel80', 'pixel81', 'pixel82', 'pixel83', 'pixel84', 'pixel85', 'pixel86', 'pixel87', 'pixel88', 'pixel89', 'pixel90', 'pixel91', 'pixel92', 'pixel93', 'pixel94', 'pixel95', 'pixel96', 'pixel97', 'pixel98', 'pixel99', 'pixel100', 'pixel101', 'pixel102', 'pixel103', 'pixel104', 'pixel105', 'pixel106', 'pixel107', 'pixel108', 'pixel109', 'pixel110', 'pixel111', 'pixel112', 'pixel113', 'pixel114', 'pixel115', 'pixel116', 'pixel117', 'pixel118', 'pixel119', 'pixel120', 'pixel121', 'pixel122', 'pixel123', 'pixel124', 'pixel125', 'pixel126', 'pixel127', 'pixel128', 'pixel129', 'pixel130', 'pixel131', 'pixel132', 'pixel133', 'pixel134', 'pixel135', 'pixel136', 'pixel137', 'pixel138', 'pixel139', 'pixel140', 'pixel141', 'pixel142', 'pixel143', 'pixel144', 'pixel145', 'pixel146', 'pixel147', 'pixel148', 'pixel149', 'pixel150', 'pixel151', 'pixel152', 'pixel153', 'pixel154', 'pixel155', 'pixel156', 'pixel157', 'pixel158', 'pixel159', 'pixel160', 'pixel161', 'pixel162', 'pixel163', 'pixel164', 'pixel165', 'pixel166', 'pixel167', 'pixel168', 'pixel169', 'pixel170', 'pixel171', 'pixel172', 'pixel173', 'pixel174', 'pixel175', 'pixel176', 'pixel177', 'pixel178', 'pixel179', 'pixel180', 'pixel181', 'pixel182', 'pixel183', 'pixel184', 'pixel185', 'pixel186', 'pixel187', 'pixel188', 'pixel189', 'pixel190', 'pixel191', 'pixel192', 'pixel193', 'pixel194', 'pixel195', 'pixel196', 'pixel197', 'pixel198', 'pixel199', 'pixel200', 'pixel201', 'pixel202', 'pixel203', 'pixel204', 'pixel205', 'pixel206', 'pixel207', 'pixel208', 'pixel209', 'pixel210', 'pixel211', 'pixel212', 'pixel213', 'pixel214', 'pixel215', 'pixel216', 'pixel217', 'pixel218', 'pixel219', 'pixel220', 'pixel221', 'pixel222', 'pixel223', 'pixel224', 'pixel225', 'pixel226', 'pixel227', 'pixel228', 'pixel229', 'pixel230', 'pixel231', 'pixel232', 'pixel233', 'pixel234', 'pixel235', 'pixel236', 'pixel237', 'pixel238', 'pixel239', 'pixel240', 'pixel241', 'pixel242', 'pixel243', 'pixel244', 'pixel245', 'pixel246', 'pixel247', 'pixel248', 'pixel249', 'pixel250', 'pixel251', 'pixel252', 'pixel253', 'pixel254', 'pixel255', 'pixel256', 'pixel257', 'pixel258', 'pixel259', 'pixel260', 'pixel261', 'pixel262', 'pixel263', 'pixel264', 'pixel265', 'pixel266', 'pixel267', 'pixel268', 'pixel269', 'pixel270', 'pixel271', 'pixel272', 'pixel273', 'pixel274', 'pixel275', 'pixel276', 'pixel277', 'pixel278', 'pixel279', 'pixel280', 'pixel281', 'pixel282', 'pixel283', 'pixel284', 'pixel285', 'pixel286', 'pixel287', 'pixel288', 'pixel289', 'pixel290', 'pixel291', 'pixel292', 'pixel293', 'pixel294', 'pixel295', 'pixel296', 'pixel297', 'pixel298', 'pixel299', 'pixel300', 'pixel301', 'pixel302', 'pixel303', 'pixel304', 'pixel305', 'pixel306', 'pixel307', 'pixel308', 'pixel309', 'pixel310', 'pixel311', 'pixel312', 'pixel313', 'pixel314', 'pixel315', 'pixel316', 'pixel317', 'pixel318', 'pixel319', 'pixel320', 'pixel321', 'pixel322', 'pixel323', 'pixel324', 'pixel325', 'pixel326', 'pixel327', 'pixel328', 'pixel329', 'pixel330', 'pixel331', 'pixel332', 'pixel333', 'pixel334', 'pixel335', 'pixel336', 'pixel337', 'pixel338', 'pixel339', 'pixel340', 'pixel341', 'pixel342', 'pixel343', 'pixel344', 'pixel345', 'pixel346', 'pixel347', 'pixel348', 'pixel349', 'pixel350', 'pixel351', 'pixel352', 'pixel353', 'pixel354', 'pixel355', 'pixel356', 'pixel357', 'pixel358', 'pixel359', 'pixel360', 'pixel361', 'pixel362', 'pixel363', 'pixel364', 'pixel365', 'pixel366', 'pixel367', 'pixel368', 'pixel369', 'pixel370', 'pixel371', 'pixel372', 'pixel373', 'pixel374', 'pixel375', 'pixel376', 'pixel377', 'pixel378', 'pixel379', 'pixel380', 'pixel381', 'pixel382', 'pixel383', 'pixel384', 'pixel385', 'pixel386', 'pixel387', 'pixel388', 'pixel389', 'pixel390', 'pixel391', 'pixel392', 'pixel393', 'pixel394', 'pixel395', 'pixel396', 'pixel397', 'pixel398', 'pixel399', 'pixel400', 'pixel401', 'pixel402', 'pixel403', 'pixel404', 'pixel405', 'pixel406', 'pixel407', 'pixel408', 'pixel409', 'pixel410', 'pixel411', 'pixel412', 'pixel413', 'pixel414', 'pixel415', 'pixel416', 'pixel417', 'pixel418', 'pixel419', 'pixel420', 'pixel421', 'pixel422', 'pixel423', 'pixel424', 'pixel425', 'pixel426', 'pixel427', 'pixel428', 'pixel429', 'pixel430', 'pixel431', 'pixel432', 'pixel433', 'pixel434', 'pixel435', 'pixel436', 'pixel437', 'pixel438', 'pixel439', 'pixel440', 'pixel441', 'pixel442', 'pixel443', 'pixel444', 'pixel445', 'pixel446', 'pixel447', 'pixel448', 'pixel449', 'pixel450', 'pixel451', 'pixel452', 'pixel453', 'pixel454', 'pixel455', 'pixel456', 'pixel457', 'pixel458', 'pixel459', 'pixel460', 'pixel461', 'pixel462', 'pixel463', 'pixel464', 'pixel465', 'pixel466', 'pixel467', 'pixel468', 'pixel469', 'pixel470', 'pixel471', 'pixel472', 'pixel473', 'pixel474', 'pixel475', 'pixel476', 'pixel477', 'pixel478', 'pixel479', 'pixel480', 'pixel481', 'pixel482', 'pixel483', 'pixel484', 'pixel485', 'pixel486', 'pixel487', 'pixel488', 'pixel489', 'pixel490', 'pixel491', 'pixel492', 'pixel493', 'pixel494', 'pixel495', 'pixel496', 'pixel497', 'pixel498', 'pixel499', 'pixel500', 'pixel501', 'pixel502', 'pixel503', 'pixel504', 'pixel505', 'pixel506', 'pixel507', 'pixel508', 'pixel509', 'pixel510', 'pixel511', 'pixel512', 'pixel513', 'pixel514', 'pixel515', 'pixel516', 'pixel517', 'pixel518', 'pixel519', 'pixel520', 'pixel521', 'pixel522', 'pixel523', 'pixel524', 'pixel525', 'pixel526', 'pixel527', 'pixel528', 'pixel529', 'pixel530', 'pixel531', 'pixel532', 'pixel533', 'pixel534', 'pixel535', 'pixel536', 'pixel537', 'pixel538', 'pixel539', 'pixel540', 'pixel541', 'pixel542', 'pixel543', 'pixel544', 'pixel545', 'pixel546', 'pixel547', 'pixel548', 'pixel549', 'pixel550', 'pixel551', 'pixel552', 'pixel553', 'pixel554', 'pixel555', 'pixel556', 'pixel557', 'pixel558', 'pixel559', 'pixel560', 'pixel561', 'pixel562', 'pixel563', 'pixel564', 'pixel565', 'pixel566', 'pixel567', 'pixel568', 'pixel569', 'pixel570', 'pixel571', 'pixel572', 'pixel573', 'pixel574', 'pixel575', 'pixel576', 'pixel577', 'pixel578', 'pixel579', 'pixel580', 'pixel581', 'pixel582', 'pixel583', 'pixel584', 'pixel585', 'pixel586', 'pixel587', 'pixel588', 'pixel589', 'pixel590', 'pixel591', 'pixel592', 'pixel593', 'pixel594', 'pixel595', 'pixel596', 'pixel597', 'pixel598', 'pixel599', 'pixel600', 'pixel601', 'pixel602', 'pixel603', 'pixel604', 'pixel605', 'pixel606', 'pixel607', 'pixel608', 'pixel609', 'pixel610', 'pixel611', 'pixel612', 'pixel613', 'pixel614', 'pixel615', 'pixel616', 'pixel617', 'pixel618', 'pixel619', 'pixel620', 'pixel621', 'pixel622', 'pixel623', 'pixel624', 'pixel625', 'pixel626', 'pixel627', 'pixel628', 'pixel629', 'pixel630', 'pixel631', 'pixel632', 'pixel633', 'pixel634', 'pixel635', 'pixel636', 'pixel637', 'pixel638', 'pixel639', 'pixel640', 'pixel641', 'pixel642', 'pixel643', 'pixel644', 'pixel645', 'pixel646', 'pixel647', 'pixel648', 'pixel649', 'pixel650', 'pixel651', 'pixel652', 'pixel653', 'pixel654', 'pixel655', 'pixel656', 'pixel657', 'pixel658', 'pixel659', 'pixel660', 'pixel661', 'pixel662', 'pixel663', 'pixel664', 'pixel665', 'pixel666', 'pixel667', 'pixel668', 'pixel669', 'pixel670', 'pixel671', 'pixel672', 'pixel673', 'pixel674', 'pixel675', 'pixel676', 'pixel677', 'pixel678', 'pixel679', 'pixel680', 'pixel681', 'pixel682', 'pixel683', 'pixel684', 'pixel685', 'pixel686', 'pixel687', 'pixel688', 'pixel689', 'pixel690', 'pixel691', 'pixel692', 'pixel693', 'pixel694', 'pixel695', 'pixel696', 'pixel697', 'pixel698', 'pixel699', 'pixel700', 'pixel701', 'pixel702', 'pixel703', 'pixel704', 'pixel705', 'pixel706', 'pixel707', 'pixel708', 'pixel709', 'pixel710', 'pixel711', 'pixel712', 'pixel713', 'pixel714', 'pixel715', 'pixel716', 'pixel717', 'pixel718', 'pixel719', 'pixel720', 'pixel721', 'pixel722', 'pixel723', 'pixel724', 'pixel725', 'pixel726', 'pixel727', 'pixel728', 'pixel729', 'pixel730', 'pixel731', 'pixel732', 'pixel733', 'pixel734', 'pixel735', 'pixel736', 'pixel737', 'pixel738', 'pixel739', 'pixel740', 'pixel741', 'pixel742', 'pixel743', 'pixel744', 'pixel745', 'pixel746', 'pixel747', 'pixel748', 'pixel749', 'pixel750', 'pixel751', 'pixel752', 'pixel753', 'pixel754', 'pixel755', 'pixel756', 'pixel757', 'pixel758', 'pixel759', 'pixel760', 'pixel761', 'pixel762', 'pixel763', 'pixel764', 'pixel765', 'pixel766', 'pixel767', 'pixel768', 'pixel769', 'pixel770', 'pixel771', 'pixel772', 'pixel773', 'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779', 'pixel780', 'pixel781', 'pixel782', 'pixel783', 'pixel784'] ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624', '625', '626', '627', '628', '629', '630', '631', '632', '633', '634', '635', '636', '637', '638', '639', '640', '641', '642', '643', '644', '645', '646', '647', '648', '649', '650', '651', '652', '653', '654', '655', '656', '657', '658', '659', '660', '661', '662', '663', '664', '665', '666', '667', '668', '669', '670', '671', '672', '673', '674', '675', '676', '677', '678', '679', '680', '681', '682', '683', '684', '685', '686', '687', '688', '689', '690', '691', '692', '693', '694', '695', '696', '697', '698', '699', '700', '701', '702', '703', '704', '705', '706', '707', '708', '709', '710', '711', '712', '713', '714', '715', '716', '717', '718', '719', '720', '721', '722', '723', '724', '725', '726', '727', '728', '729', '730', '731', '732', '733', '734', '735', '736', '737', '738', '739', '740', '741', '742', '743', '744', '745', '746', '747', '748', '749', '750', '751', '752', '753', '754', '755', '756', '757', '758', '759', '760', '761', '762', '763', '764', '765', '766', '767', '768', '769', '770', '771', '772', '773', '774', '775', '776', '777', '778', '779', '780', '781', '782', '783']\nexpected pixel246, pixel735, pixel484, pixel54, pixel741, pixel126, pixel411, pixel302, pixel296, pixel3, pixel55, pixel172, pixel747, pixel287, pixel77, pixel309, pixel558, pixel755, pixel168, pixel290, pixel37, pixel383, pixel127, pixel26, pixel137, pixel458, pixel410, pixel260, pixel528, pixel583, pixel474, pixel695, pixel485, pixel507, pixel180, pixel731, pixel545, pixel162, pixel456, pixel202, pixel526, pixel693, pixel527, pixel293, pixel169, pixel654, pixel228, pixel762, pixel38, pixel376, pixel664, pixel164, pixel110, pixel259, pixel208, pixel760, pixel720, pixel173, pixel1, pixel14, pixel380, pixel708, pixel159, pixel166, pixel512, pixel234, pixel245, pixel282, pixel473, pixel308, pixel4, pixel509, pixel602, pixel751, pixel257, pixel352, pixel597, pixel745, pixel549, pixel101, pixel661, pixel600, pixel576, pixel87, pixel675, pixel610, pixel777, pixel525, pixel278, pixel45, pixel590, pixel412, pixel683, pixel562, pixel739, pixel443, pixel454, pixel274, pixel58, pixel361, pixel397, pixel408, pixel536, pixel727, pixel165, pixel501, pixel334, pixel687, pixel649, pixel633, pixel716, pixel183, pixel382, pixel364, pixel270, pixel340, pixel131, pixel419, pixel697, pixel253, pixel706, pixel707, pixel343, pixel52, pixel324, pixel345, pixel470, pixel135, pixel582, pixel608, pixel710, pixel242, pixel123, pixel73, pixel753, pixel86, pixel221, pixel547, pixel552, pixel556, pixel206, pixel357, pixel385, pixel150, pixel457, pixel120, pixel394, pixel205, pixel617, pixel564, pixel303, pixel551, pixel424, pixel190, pixel479, pixel724, pixel646, pixel262, pixel311, pixel243, pixel415, pixel182, pixel503, pixel330, pixel427, pixel709, pixel367, pixel506, pixel681, pixel765, pixel279, pixel188, pixel323, pixel591, pixel658, pixel767, pixel217, pixel401, pixel374, pixel36, pixel669, pixel481, pixel100, pixel404, pixel631, pixel422, pixel524, pixel603, pixel75, pixel599, pixel770, pixel402, pixel344, pixel449, pixel12, pixel594, pixel151, pixel768, pixel124, pixel10, pixel212, pixel426, pixel390, pixel160, pixel694, pixel115, pixel226, pixel265, pixel680, pixel472, pixel435, pixel158, pixel15, pixel333, pixel548, pixel758, pixel715, pixel273, pixel378, pixel643, pixel660, pixel671, pixel391, pixel780, pixel218, pixel222, pixel563, pixel436, pixel29, pixel754, pixel121, pixel289, pixel555, pixel469, pixel275, pixel757, pixel49, pixel644, pixel476, pixel587, pixel752, pixel573, pixel142, pixel738, pixel451, pixel41, pixel240, pixel25, pixel448, pixel11, pixel392, pixel504, pixel276, pixel580, pixel34, pixel518, pixel645, pixel534, pixel450, pixel618, pixel97, pixel7, pixel230, pixel154, pixel197, pixel71, pixel569, pixel201, pixel141, pixel194, pixel482, pixel588, pixel533, pixel62, pixel157, pixel516, pixel163, pixel539, pixel607, pixel696, pixel310, pixel519, pixel195, pixel2, pixel537, pixel258, pixel637, pixel441, pixel650, pixel338, pixel360, pixel138, pixel109, pixel277, pixel152, pixel586, pixel372, pixel440, pixel766, pixel523, pixel744, pixel736, pixel414, pixel365, pixel139, pixel430, pixel196, pixel74, pixel488, pixel493, pixel8, pixel420, pixel585, pixel620, pixel149, pixel691, pixel91, pixel418, pixel657, pixel104, pixel31, pixel656, pixel437, pixel143, pixel769, pixel35, pixel626, pixel269, pixel285, pixel388, pixel783, pixel743, pixel70, pixel211, pixel329, pixel495, pixel237, pixel187, pixel726, pixel98, pixel178, pixel508, pixel223, pixel94, pixel155, pixel718, pixel530, pixel95, pixel778, pixel566, pixel439, pixel468, pixel379, pixel300, pixel659, pixel465, pixel375, pixel297, pixel146, pixel161, pixel167, pixel764, pixel215, pixel433, pixel670, pixel553, pixel763, pixel17, pixel517, pixel595, pixel612, pixel613, pixel489, pixel81, pixel748, pixel292, pixel425, pixel347, pixel499, pixel611, pixel116, pixel46, pixel492, pixel96, pixel6, pixel624, pixel640, pixel306, pixel704, pixel337, pixel728, pixel241, pixel462, pixel288, pixel759, pixel103, pixel118, pixel174, pixel27, pixel39, pixel114, pixel299, pixel604, pixel505, pixel171, pixel446, pixel261, pixel328, pixel616, pixel546, pixel327, pixel722, pixel596, pixel711, pixel67, pixel636, pixel16, pixel184, pixel315, pixel362, pixel214, pixel638, pixel730, pixel122, pixel341, pixel351, pixel381, pixel342, pixel452, pixel203, pixel749, pixel318, pixel714, pixel339, pixel559, pixel298, pixel319, pixel354, pixel779, pixel483, pixel550, pixel147, pixel575, pixel92, pixel699, pixel199, pixel672, pixel113, pixel455, pixel574, pixel677, pixel405, pixel271, pixel428, pixel57, pixel561, pixel331, pixel464, pixel248, pixel170, pixel320, pixel181, pixel129, pixel702, pixel614, pixel209, pixel128, pixel407, pixel305, pixel386, pixel511, pixel560, pixel369, pixel421, pixel247, pixel776, pixel629, pixel368, pixel625, pixel653, pixel134, pixel56, pixel632, pixel193, pixel236, pixel690, pixel225, pixel266, pixel486, pixel554, pixel153, pixel175, pixel50, pixel363, pixel185, pixel413, pixel491, pixel572, pixel471, pixel647, pixel280, pixel281, pixel18, pixel689, pixel301, pixel445, pixel682, pixel78, pixel746, pixel350, pixel53, pixel366, pixel295, pixel177, pixel72, pixel264, pixel99, pixel674, pixel85, pixel522, pixel592, pixel42, pixel497, pixel557, pixel399, pixel89, pixel348, pixel21, pixel117, pixel249, pixel584, pixel387, pixel510, pixel80, pixel355, pixel453, pixel5, pixel712, pixel59, pixel761, pixel538, pixel466, pixel679, pixel13, pixel700, pixel500, pixel251, pixel775, pixel32, pixel729, pixel634, pixel773, pixel772, pixel447, pixel353, pixel570, pixel61, pixel544, pixel685, pixel434, pixel395, pixel30, pixel498, pixel673, pixel24, pixel438, pixel665, pixel723, pixel416, pixel346, pixel286, pixel325, pixel66, pixel349, pixel256, pixel432, pixel467, pixel601, pixel406, pixel112, pixel578, pixel737, pixel106, pixel244, pixel140, pixel444, pixel254, pixel698, pixel701, pixel480, pixel487, pixel312, pixel781, pixel642, pixel284, pixel216, pixel398, pixel317, pixel359, pixel565, pixel79, pixel102, pixel272, pixel515, pixel316, pixel589, pixel667, pixel64, pixel105, pixel90, pixel373, pixel567, pixel400, pixel204, pixel579, pixel635, pixel496, pixel48, pixel84, pixel322, pixel541, pixel771, pixel250, pixel393, pixel130, pixel684, pixel717, pixel179, pixel571, pixel207, pixel371, pixel417, pixel83, pixel255, pixel459, pixel666, pixel540, pixel336, pixel732, pixel377, pixel577, pixel283, pixel396, pixel133, pixel403, pixel639, pixel581, pixel721, pixel543, pixel461, pixel88, pixel692, pixel108, pixel389, pixel686, pixel502, pixel65, pixel431, pixel409, pixel478, pixel703, pixel111, pixel605, pixel648, pixel622, pixel219, pixel125, pixel652, pixel623, pixel460, pixel676, pixel606, pixel314, pixel630, pixel663, pixel9, pixel593, pixel40, pixel252, pixel358, pixel198, pixel136, pixel494, pixel725, pixel641, pixel238, pixel33, pixel532, pixel750, pixel477, pixel542, pixel313, pixel734, pixel76, pixel662, pixel200, pixel82, pixel28, pixel370, pixel520, pixel490, pixel521, pixel784, pixel68, pixel23, pixel186, pixel132, pixel627, pixel463, pixel227, pixel291, pixel189, pixel231, pixel176, pixel535, pixel668, pixel678, pixel294, pixel267, pixel44, pixel651, pixel513, pixel239, pixel475, pixel514, pixel531, pixel220, pixel321, pixel51, pixel733, pixel156, pixel210, pixel628, pixel326, pixel782, pixel47, pixel148, pixel688, pixel335, pixel621, pixel144, pixel22, pixel307, pixel233, pixel93, pixel705, pixel268, pixel192, pixel229, pixel615, pixel43, pixel60, pixel63, pixel191, pixel145, pixel304, pixel332, pixel598, pixel655, pixel719, pixel742, pixel107, pixel263, pixel356, pixel740, pixel429, pixel235, pixel384, pixel224, pixel19, pixel232, pixel756, pixel423, pixel529, pixel568, pixel713, pixel20, pixel119, pixel69, pixel442, pixel609, pixel774, pixel619, pixel213 in input data\ntraining data did not have the following fields: 298, 468, 641, 44, 140, 51, 323, 246, 745, 263, 252, 659, 663, 341, 657, 572, 312, 473, 431, 101, 187, 489, 137, 491, 735, 31, 481, 64, 71, 90, 145, 345, 452, 268, 691, 783, 470, 378, 496, 592, 11, 500, 731, 146, 732, 475, 535, 206, 319, 778, 507, 518, 571, 423, 618, 747, 435, 46, 362, 560, 122, 669, 543, 84, 121, 248, 654, 164, 410, 613, 47, 79, 42, 247, 308, 594, 397, 225, 465, 484, 670, 428, 276, 545, 433, 579, 147, 482, 608, 704, 265, 463, 182, 102, 462, 212, 392, 352, 56, 293, 421, 38, 17, 597, 719, 551, 537, 708, 509, 151, 635, 504, 529, 94, 395, 602, 774, 779, 237, 513, 236, 184, 497, 750, 324, 70, 721, 580, 110, 4, 662, 239, 710, 619, 9, 399, 416, 143, 255, 93, 652, 359, 253, 531, 224, 379, 598, 148, 35, 374, 727, 675, 458, 183, 632, 767, 630, 55, 589, 180, 205, 479, 400, 36, 586, 106, 375, 440, 559, 208, 251, 202, 536, 174, 278, 177, 372, 695, 765, 769, 289, 2, 6, 398, 425, 457, 404, 587, 777, 764, 616, 689, 66, 582, 132, 681, 29, 108, 746, 92, 367, 171, 665, 192, 664, 627, 684, 771, 5, 653, 711, 544, 307, 568, 18, 553, 539, 756, 739, 558, 384, 326, 649, 775, 111, 259, 306, 170, 321, 644, 660, 688, 344, 759, 439, 776, 215, 591, 685, 347, 549, 209, 492, 136, 130, 100, 709, 371, 37, 157, 722, 476, 361, 385, 131, 261, 603, 28, 698, 103, 305, 542, 112, 257, 309, 340, 493, 350, 426, 126, 301, 437, 227, 631, 782, 483, 201, 338, 501, 737, 754, 510, 624, 526, 525, 524, 533, 48, 226, 144, 115, 152, 561, 628, 279, 211, 472, 254, 240, 295, 455, 738, 322, 574, 235, 99, 172, 381, 169, 499, 216, 292, 346, 459, 314, 61, 30, 498, 554, 40, 186, 142, 508, 581, 393, 625, 286, 383, 730, 601, 43, 358, 523, 612, 302, 179, 717, 68, 188, 194, 267, 658, 720, 636, 58, 272, 281, 325, 673, 116, 780, 683, 105, 363, 69, 24, 32, 615, 471, 766, 19, 195, 10, 274, 450, 736, 677, 85, 505, 124, 387, 76, 149, 16, 588, 693, 277, 377, 487, 303, 570, 573, 369, 406, 578, 418, 141, 637, 73, 368, 173, 394, 577, 694, 349, 449, 128, 65, 517, 600, 502, 438, 223, 490, 310, 622, 762, 315, 611, 20, 161, 469, 733, 667, 113, 156, 605, 360, 198, 207, 755, 327, 380, 297, 299, 80, 456, 486, 366, 528, 599, 353, 718, 585, 59, 728, 222, 734, 196, 336, 420, 330, 503, 485, 342, 445, 506, 97, 520, 193, 318, 74, 724, 91, 49, 564, 444, 107, 23, 155, 389, 467, 382, 7, 133, 453, 419, 191, 538, 661, 626, 781, 758, 557, 52, 567, 27, 488, 726, 370, 770, 82, 723, 162, 221, 34, 104, 114, 77, 495, 519, 646, 402, 740, 595, 415, 680, 134, 288, 687, 751, 3, 427, 390, 696, 291, 39, 575, 109, 217, 446, 165, 95, 22, 166, 552, 118, 316, 436, 26, 75, 511, 742, 21, 262, 556, 638, 623, 666, 712, 335, 703, 443, 407, 411, 332, 701, 200, 123, 331, 583, 13, 269, 682, 238, 60, 178, 210, 593, 287, 320, 464, 391, 203, 692, 768, 86, 477, 606, 668, 707, 199, 729, 639, 773, 160, 743, 67, 228, 294, 725, 474, 386, 167, 244, 633, 466, 15, 749, 150, 78, 584, 676, 590, 14, 548, 220, 541, 555, 429, 283, 715, 204, 540, 617, 376, 355, 671, 245, 412, 189, 748, 563, 679, 273, 280, 614, 163, 642, 700, 311, 645, 434, 354, 232, 643, 96, 83, 396, 515, 546, 328, 647, 12, 337, 213, 334, 648, 621, 271, 285, 357, 454, 460, 451, 98, 348, 364, 757, 218, 290, 54, 702, 313, 190, 234, 266, 63, 270, 413, 154, 138, 512, 596, 339, 424, 153, 8, 181, 772, 744, 87, 0, 478, 135, 230, 275, 296, 516, 655, 250, 409, 514, 494, 229, 231, 405, 530, 62, 351, 284, 752, 741, 562, 656, 41, 753, 159, 566, 686, 401, 569, 50, 125, 120, 761, 417, 408, 480, 714, 197, 422, 607, 651, 158, 356, 532, 176, 373, 300, 521, 264, 343, 634, 304, 45, 388, 697, 33, 127, 610, 139, 317, 53, 547, 550, 763, 760, 705, 690, 214, 713, 461, 25, 119, 242, 672, 576, 88, 249, 522, 260, 620, 185, 81, 650, 57, 565, 333, 258, 233, 442, 365, 447, 168, 72, 219, 241, 256, 448, 629, 716, 699, 1, 329, 527, 640, 678, 432, 282, 674, 175, 129, 403, 441, 609, 430, 243, 117, 414, 706, 534, 604, 89",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-289-c598b1086c26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mnew_y_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_X_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, validate_features, base_margin)\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mntree_limit\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mntree_limit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"best_ntree_limit\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m         class_probs = self.get_booster().predict(\n\u001b[0m\u001b[0;32m    971\u001b[0m             \u001b[0mtest_dmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features, training)\u001b[0m\n\u001b[0;32m   1483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m         \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2058\u001b[0m                             ', '.join(str(s) for s in my_missing))\n\u001b[0;32m   2059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2060\u001b[1;33m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0m\u001b[0;32m   2061\u001b[0m                                             data.feature_names))\n\u001b[0;32m   2062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names mismatch: ['pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6', 'pixel7', 'pixel8', 'pixel9', 'pixel10', 'pixel11', 'pixel12', 'pixel13', 'pixel14', 'pixel15', 'pixel16', 'pixel17', 'pixel18', 'pixel19', 'pixel20', 'pixel21', 'pixel22', 'pixel23', 'pixel24', 'pixel25', 'pixel26', 'pixel27', 'pixel28', 'pixel29', 'pixel30', 'pixel31', 'pixel32', 'pixel33', 'pixel34', 'pixel35', 'pixel36', 'pixel37', 'pixel38', 'pixel39', 'pixel40', 'pixel41', 'pixel42', 'pixel43', 'pixel44', 'pixel45', 'pixel46', 'pixel47', 'pixel48', 'pixel49', 'pixel50', 'pixel51', 'pixel52', 'pixel53', 'pixel54', 'pixel55', 'pixel56', 'pixel57', 'pixel58', 'pixel59', 'pixel60', 'pixel61', 'pixel62', 'pixel63', 'pixel64', 'pixel65', 'pixel66', 'pixel67', 'pixel68', 'pixel69', 'pixel70', 'pixel71', 'pixel72', 'pixel73', 'pixel74', 'pixel75', 'pixel76', 'pixel77', 'pixel78', 'pixel79', 'pixel80', 'pixel81', 'pixel82', 'pixel83', 'pixel84', 'pixel85', 'pixel86', 'pixel87', 'pixel88', 'pixel89', 'pixel90', 'pixel91', 'pixel92', 'pixel93', 'pixel94', 'pixel95', 'pixel96', 'pixel97', 'pixel98', 'pixel99', 'pixel100', 'pixel101', 'pixel102', 'pixel103', 'pixel104', 'pixel105', 'pixel106', 'pixel107', 'pixel108', 'pixel109', 'pixel110', 'pixel111', 'pixel112', 'pixel113', 'pixel114', 'pixel115', 'pixel116', 'pixel117', 'pixel118', 'pixel119', 'pixel120', 'pixel121', 'pixel122', 'pixel123', 'pixel124', 'pixel125', 'pixel126', 'pixel127', 'pixel128', 'pixel129', 'pixel130', 'pixel131', 'pixel132', 'pixel133', 'pixel134', 'pixel135', 'pixel136', 'pixel137', 'pixel138', 'pixel139', 'pixel140', 'pixel141', 'pixel142', 'pixel143', 'pixel144', 'pixel145', 'pixel146', 'pixel147', 'pixel148', 'pixel149', 'pixel150', 'pixel151', 'pixel152', 'pixel153', 'pixel154', 'pixel155', 'pixel156', 'pixel157', 'pixel158', 'pixel159', 'pixel160', 'pixel161', 'pixel162', 'pixel163', 'pixel164', 'pixel165', 'pixel166', 'pixel167', 'pixel168', 'pixel169', 'pixel170', 'pixel171', 'pixel172', 'pixel173', 'pixel174', 'pixel175', 'pixel176', 'pixel177', 'pixel178', 'pixel179', 'pixel180', 'pixel181', 'pixel182', 'pixel183', 'pixel184', 'pixel185', 'pixel186', 'pixel187', 'pixel188', 'pixel189', 'pixel190', 'pixel191', 'pixel192', 'pixel193', 'pixel194', 'pixel195', 'pixel196', 'pixel197', 'pixel198', 'pixel199', 'pixel200', 'pixel201', 'pixel202', 'pixel203', 'pixel204', 'pixel205', 'pixel206', 'pixel207', 'pixel208', 'pixel209', 'pixel210', 'pixel211', 'pixel212', 'pixel213', 'pixel214', 'pixel215', 'pixel216', 'pixel217', 'pixel218', 'pixel219', 'pixel220', 'pixel221', 'pixel222', 'pixel223', 'pixel224', 'pixel225', 'pixel226', 'pixel227', 'pixel228', 'pixel229', 'pixel230', 'pixel231', 'pixel232', 'pixel233', 'pixel234', 'pixel235', 'pixel236', 'pixel237', 'pixel238', 'pixel239', 'pixel240', 'pixel241', 'pixel242', 'pixel243', 'pixel244', 'pixel245', 'pixel246', 'pixel247', 'pixel248', 'pixel249', 'pixel250', 'pixel251', 'pixel252', 'pixel253', 'pixel254', 'pixel255', 'pixel256', 'pixel257', 'pixel258', 'pixel259', 'pixel260', 'pixel261', 'pixel262', 'pixel263', 'pixel264', 'pixel265', 'pixel266', 'pixel267', 'pixel268', 'pixel269', 'pixel270', 'pixel271', 'pixel272', 'pixel273', 'pixel274', 'pixel275', 'pixel276', 'pixel277', 'pixel278', 'pixel279', 'pixel280', 'pixel281', 'pixel282', 'pixel283', 'pixel284', 'pixel285', 'pixel286', 'pixel287', 'pixel288', 'pixel289', 'pixel290', 'pixel291', 'pixel292', 'pixel293', 'pixel294', 'pixel295', 'pixel296', 'pixel297', 'pixel298', 'pixel299', 'pixel300', 'pixel301', 'pixel302', 'pixel303', 'pixel304', 'pixel305', 'pixel306', 'pixel307', 'pixel308', 'pixel309', 'pixel310', 'pixel311', 'pixel312', 'pixel313', 'pixel314', 'pixel315', 'pixel316', 'pixel317', 'pixel318', 'pixel319', 'pixel320', 'pixel321', 'pixel322', 'pixel323', 'pixel324', 'pixel325', 'pixel326', 'pixel327', 'pixel328', 'pixel329', 'pixel330', 'pixel331', 'pixel332', 'pixel333', 'pixel334', 'pixel335', 'pixel336', 'pixel337', 'pixel338', 'pixel339', 'pixel340', 'pixel341', 'pixel342', 'pixel343', 'pixel344', 'pixel345', 'pixel346', 'pixel347', 'pixel348', 'pixel349', 'pixel350', 'pixel351', 'pixel352', 'pixel353', 'pixel354', 'pixel355', 'pixel356', 'pixel357', 'pixel358', 'pixel359', 'pixel360', 'pixel361', 'pixel362', 'pixel363', 'pixel364', 'pixel365', 'pixel366', 'pixel367', 'pixel368', 'pixel369', 'pixel370', 'pixel371', 'pixel372', 'pixel373', 'pixel374', 'pixel375', 'pixel376', 'pixel377', 'pixel378', 'pixel379', 'pixel380', 'pixel381', 'pixel382', 'pixel383', 'pixel384', 'pixel385', 'pixel386', 'pixel387', 'pixel388', 'pixel389', 'pixel390', 'pixel391', 'pixel392', 'pixel393', 'pixel394', 'pixel395', 'pixel396', 'pixel397', 'pixel398', 'pixel399', 'pixel400', 'pixel401', 'pixel402', 'pixel403', 'pixel404', 'pixel405', 'pixel406', 'pixel407', 'pixel408', 'pixel409', 'pixel410', 'pixel411', 'pixel412', 'pixel413', 'pixel414', 'pixel415', 'pixel416', 'pixel417', 'pixel418', 'pixel419', 'pixel420', 'pixel421', 'pixel422', 'pixel423', 'pixel424', 'pixel425', 'pixel426', 'pixel427', 'pixel428', 'pixel429', 'pixel430', 'pixel431', 'pixel432', 'pixel433', 'pixel434', 'pixel435', 'pixel436', 'pixel437', 'pixel438', 'pixel439', 'pixel440', 'pixel441', 'pixel442', 'pixel443', 'pixel444', 'pixel445', 'pixel446', 'pixel447', 'pixel448', 'pixel449', 'pixel450', 'pixel451', 'pixel452', 'pixel453', 'pixel454', 'pixel455', 'pixel456', 'pixel457', 'pixel458', 'pixel459', 'pixel460', 'pixel461', 'pixel462', 'pixel463', 'pixel464', 'pixel465', 'pixel466', 'pixel467', 'pixel468', 'pixel469', 'pixel470', 'pixel471', 'pixel472', 'pixel473', 'pixel474', 'pixel475', 'pixel476', 'pixel477', 'pixel478', 'pixel479', 'pixel480', 'pixel481', 'pixel482', 'pixel483', 'pixel484', 'pixel485', 'pixel486', 'pixel487', 'pixel488', 'pixel489', 'pixel490', 'pixel491', 'pixel492', 'pixel493', 'pixel494', 'pixel495', 'pixel496', 'pixel497', 'pixel498', 'pixel499', 'pixel500', 'pixel501', 'pixel502', 'pixel503', 'pixel504', 'pixel505', 'pixel506', 'pixel507', 'pixel508', 'pixel509', 'pixel510', 'pixel511', 'pixel512', 'pixel513', 'pixel514', 'pixel515', 'pixel516', 'pixel517', 'pixel518', 'pixel519', 'pixel520', 'pixel521', 'pixel522', 'pixel523', 'pixel524', 'pixel525', 'pixel526', 'pixel527', 'pixel528', 'pixel529', 'pixel530', 'pixel531', 'pixel532', 'pixel533', 'pixel534', 'pixel535', 'pixel536', 'pixel537', 'pixel538', 'pixel539', 'pixel540', 'pixel541', 'pixel542', 'pixel543', 'pixel544', 'pixel545', 'pixel546', 'pixel547', 'pixel548', 'pixel549', 'pixel550', 'pixel551', 'pixel552', 'pixel553', 'pixel554', 'pixel555', 'pixel556', 'pixel557', 'pixel558', 'pixel559', 'pixel560', 'pixel561', 'pixel562', 'pixel563', 'pixel564', 'pixel565', 'pixel566', 'pixel567', 'pixel568', 'pixel569', 'pixel570', 'pixel571', 'pixel572', 'pixel573', 'pixel574', 'pixel575', 'pixel576', 'pixel577', 'pixel578', 'pixel579', 'pixel580', 'pixel581', 'pixel582', 'pixel583', 'pixel584', 'pixel585', 'pixel586', 'pixel587', 'pixel588', 'pixel589', 'pixel590', 'pixel591', 'pixel592', 'pixel593', 'pixel594', 'pixel595', 'pixel596', 'pixel597', 'pixel598', 'pixel599', 'pixel600', 'pixel601', 'pixel602', 'pixel603', 'pixel604', 'pixel605', 'pixel606', 'pixel607', 'pixel608', 'pixel609', 'pixel610', 'pixel611', 'pixel612', 'pixel613', 'pixel614', 'pixel615', 'pixel616', 'pixel617', 'pixel618', 'pixel619', 'pixel620', 'pixel621', 'pixel622', 'pixel623', 'pixel624', 'pixel625', 'pixel626', 'pixel627', 'pixel628', 'pixel629', 'pixel630', 'pixel631', 'pixel632', 'pixel633', 'pixel634', 'pixel635', 'pixel636', 'pixel637', 'pixel638', 'pixel639', 'pixel640', 'pixel641', 'pixel642', 'pixel643', 'pixel644', 'pixel645', 'pixel646', 'pixel647', 'pixel648', 'pixel649', 'pixel650', 'pixel651', 'pixel652', 'pixel653', 'pixel654', 'pixel655', 'pixel656', 'pixel657', 'pixel658', 'pixel659', 'pixel660', 'pixel661', 'pixel662', 'pixel663', 'pixel664', 'pixel665', 'pixel666', 'pixel667', 'pixel668', 'pixel669', 'pixel670', 'pixel671', 'pixel672', 'pixel673', 'pixel674', 'pixel675', 'pixel676', 'pixel677', 'pixel678', 'pixel679', 'pixel680', 'pixel681', 'pixel682', 'pixel683', 'pixel684', 'pixel685', 'pixel686', 'pixel687', 'pixel688', 'pixel689', 'pixel690', 'pixel691', 'pixel692', 'pixel693', 'pixel694', 'pixel695', 'pixel696', 'pixel697', 'pixel698', 'pixel699', 'pixel700', 'pixel701', 'pixel702', 'pixel703', 'pixel704', 'pixel705', 'pixel706', 'pixel707', 'pixel708', 'pixel709', 'pixel710', 'pixel711', 'pixel712', 'pixel713', 'pixel714', 'pixel715', 'pixel716', 'pixel717', 'pixel718', 'pixel719', 'pixel720', 'pixel721', 'pixel722', 'pixel723', 'pixel724', 'pixel725', 'pixel726', 'pixel727', 'pixel728', 'pixel729', 'pixel730', 'pixel731', 'pixel732', 'pixel733', 'pixel734', 'pixel735', 'pixel736', 'pixel737', 'pixel738', 'pixel739', 'pixel740', 'pixel741', 'pixel742', 'pixel743', 'pixel744', 'pixel745', 'pixel746', 'pixel747', 'pixel748', 'pixel749', 'pixel750', 'pixel751', 'pixel752', 'pixel753', 'pixel754', 'pixel755', 'pixel756', 'pixel757', 'pixel758', 'pixel759', 'pixel760', 'pixel761', 'pixel762', 'pixel763', 'pixel764', 'pixel765', 'pixel766', 'pixel767', 'pixel768', 'pixel769', 'pixel770', 'pixel771', 'pixel772', 'pixel773', 'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779', 'pixel780', 'pixel781', 'pixel782', 'pixel783', 'pixel784'] ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624', '625', '626', '627', '628', '629', '630', '631', '632', '633', '634', '635', '636', '637', '638', '639', '640', '641', '642', '643', '644', '645', '646', '647', '648', '649', '650', '651', '652', '653', '654', '655', '656', '657', '658', '659', '660', '661', '662', '663', '664', '665', '666', '667', '668', '669', '670', '671', '672', '673', '674', '675', '676', '677', '678', '679', '680', '681', '682', '683', '684', '685', '686', '687', '688', '689', '690', '691', '692', '693', '694', '695', '696', '697', '698', '699', '700', '701', '702', '703', '704', '705', '706', '707', '708', '709', '710', '711', '712', '713', '714', '715', '716', '717', '718', '719', '720', '721', '722', '723', '724', '725', '726', '727', '728', '729', '730', '731', '732', '733', '734', '735', '736', '737', '738', '739', '740', '741', '742', '743', '744', '745', '746', '747', '748', '749', '750', '751', '752', '753', '754', '755', '756', '757', '758', '759', '760', '761', '762', '763', '764', '765', '766', '767', '768', '769', '770', '771', '772', '773', '774', '775', '776', '777', '778', '779', '780', '781', '782', '783']\nexpected pixel246, pixel735, pixel484, pixel54, pixel741, pixel126, pixel411, pixel302, pixel296, pixel3, pixel55, pixel172, pixel747, pixel287, pixel77, pixel309, pixel558, pixel755, pixel168, pixel290, pixel37, pixel383, pixel127, pixel26, pixel137, pixel458, pixel410, pixel260, pixel528, pixel583, pixel474, pixel695, pixel485, pixel507, pixel180, pixel731, pixel545, pixel162, pixel456, pixel202, pixel526, pixel693, pixel527, pixel293, pixel169, pixel654, pixel228, pixel762, pixel38, pixel376, pixel664, pixel164, pixel110, pixel259, pixel208, pixel760, pixel720, pixel173, pixel1, pixel14, pixel380, pixel708, pixel159, pixel166, pixel512, pixel234, pixel245, pixel282, pixel473, pixel308, pixel4, pixel509, pixel602, pixel751, pixel257, pixel352, pixel597, pixel745, pixel549, pixel101, pixel661, pixel600, pixel576, pixel87, pixel675, pixel610, pixel777, pixel525, pixel278, pixel45, pixel590, pixel412, pixel683, pixel562, pixel739, pixel443, pixel454, pixel274, pixel58, pixel361, pixel397, pixel408, pixel536, pixel727, pixel165, pixel501, pixel334, pixel687, pixel649, pixel633, pixel716, pixel183, pixel382, pixel364, pixel270, pixel340, pixel131, pixel419, pixel697, pixel253, pixel706, pixel707, pixel343, pixel52, pixel324, pixel345, pixel470, pixel135, pixel582, pixel608, pixel710, pixel242, pixel123, pixel73, pixel753, pixel86, pixel221, pixel547, pixel552, pixel556, pixel206, pixel357, pixel385, pixel150, pixel457, pixel120, pixel394, pixel205, pixel617, pixel564, pixel303, pixel551, pixel424, pixel190, pixel479, pixel724, pixel646, pixel262, pixel311, pixel243, pixel415, pixel182, pixel503, pixel330, pixel427, pixel709, pixel367, pixel506, pixel681, pixel765, pixel279, pixel188, pixel323, pixel591, pixel658, pixel767, pixel217, pixel401, pixel374, pixel36, pixel669, pixel481, pixel100, pixel404, pixel631, pixel422, pixel524, pixel603, pixel75, pixel599, pixel770, pixel402, pixel344, pixel449, pixel12, pixel594, pixel151, pixel768, pixel124, pixel10, pixel212, pixel426, pixel390, pixel160, pixel694, pixel115, pixel226, pixel265, pixel680, pixel472, pixel435, pixel158, pixel15, pixel333, pixel548, pixel758, pixel715, pixel273, pixel378, pixel643, pixel660, pixel671, pixel391, pixel780, pixel218, pixel222, pixel563, pixel436, pixel29, pixel754, pixel121, pixel289, pixel555, pixel469, pixel275, pixel757, pixel49, pixel644, pixel476, pixel587, pixel752, pixel573, pixel142, pixel738, pixel451, pixel41, pixel240, pixel25, pixel448, pixel11, pixel392, pixel504, pixel276, pixel580, pixel34, pixel518, pixel645, pixel534, pixel450, pixel618, pixel97, pixel7, pixel230, pixel154, pixel197, pixel71, pixel569, pixel201, pixel141, pixel194, pixel482, pixel588, pixel533, pixel62, pixel157, pixel516, pixel163, pixel539, pixel607, pixel696, pixel310, pixel519, pixel195, pixel2, pixel537, pixel258, pixel637, pixel441, pixel650, pixel338, pixel360, pixel138, pixel109, pixel277, pixel152, pixel586, pixel372, pixel440, pixel766, pixel523, pixel744, pixel736, pixel414, pixel365, pixel139, pixel430, pixel196, pixel74, pixel488, pixel493, pixel8, pixel420, pixel585, pixel620, pixel149, pixel691, pixel91, pixel418, pixel657, pixel104, pixel31, pixel656, pixel437, pixel143, pixel769, pixel35, pixel626, pixel269, pixel285, pixel388, pixel783, pixel743, pixel70, pixel211, pixel329, pixel495, pixel237, pixel187, pixel726, pixel98, pixel178, pixel508, pixel223, pixel94, pixel155, pixel718, pixel530, pixel95, pixel778, pixel566, pixel439, pixel468, pixel379, pixel300, pixel659, pixel465, pixel375, pixel297, pixel146, pixel161, pixel167, pixel764, pixel215, pixel433, pixel670, pixel553, pixel763, pixel17, pixel517, pixel595, pixel612, pixel613, pixel489, pixel81, pixel748, pixel292, pixel425, pixel347, pixel499, pixel611, pixel116, pixel46, pixel492, pixel96, pixel6, pixel624, pixel640, pixel306, pixel704, pixel337, pixel728, pixel241, pixel462, pixel288, pixel759, pixel103, pixel118, pixel174, pixel27, pixel39, pixel114, pixel299, pixel604, pixel505, pixel171, pixel446, pixel261, pixel328, pixel616, pixel546, pixel327, pixel722, pixel596, pixel711, pixel67, pixel636, pixel16, pixel184, pixel315, pixel362, pixel214, pixel638, pixel730, pixel122, pixel341, pixel351, pixel381, pixel342, pixel452, pixel203, pixel749, pixel318, pixel714, pixel339, pixel559, pixel298, pixel319, pixel354, pixel779, pixel483, pixel550, pixel147, pixel575, pixel92, pixel699, pixel199, pixel672, pixel113, pixel455, pixel574, pixel677, pixel405, pixel271, pixel428, pixel57, pixel561, pixel331, pixel464, pixel248, pixel170, pixel320, pixel181, pixel129, pixel702, pixel614, pixel209, pixel128, pixel407, pixel305, pixel386, pixel511, pixel560, pixel369, pixel421, pixel247, pixel776, pixel629, pixel368, pixel625, pixel653, pixel134, pixel56, pixel632, pixel193, pixel236, pixel690, pixel225, pixel266, pixel486, pixel554, pixel153, pixel175, pixel50, pixel363, pixel185, pixel413, pixel491, pixel572, pixel471, pixel647, pixel280, pixel281, pixel18, pixel689, pixel301, pixel445, pixel682, pixel78, pixel746, pixel350, pixel53, pixel366, pixel295, pixel177, pixel72, pixel264, pixel99, pixel674, pixel85, pixel522, pixel592, pixel42, pixel497, pixel557, pixel399, pixel89, pixel348, pixel21, pixel117, pixel249, pixel584, pixel387, pixel510, pixel80, pixel355, pixel453, pixel5, pixel712, pixel59, pixel761, pixel538, pixel466, pixel679, pixel13, pixel700, pixel500, pixel251, pixel775, pixel32, pixel729, pixel634, pixel773, pixel772, pixel447, pixel353, pixel570, pixel61, pixel544, pixel685, pixel434, pixel395, pixel30, pixel498, pixel673, pixel24, pixel438, pixel665, pixel723, pixel416, pixel346, pixel286, pixel325, pixel66, pixel349, pixel256, pixel432, pixel467, pixel601, pixel406, pixel112, pixel578, pixel737, pixel106, pixel244, pixel140, pixel444, pixel254, pixel698, pixel701, pixel480, pixel487, pixel312, pixel781, pixel642, pixel284, pixel216, pixel398, pixel317, pixel359, pixel565, pixel79, pixel102, pixel272, pixel515, pixel316, pixel589, pixel667, pixel64, pixel105, pixel90, pixel373, pixel567, pixel400, pixel204, pixel579, pixel635, pixel496, pixel48, pixel84, pixel322, pixel541, pixel771, pixel250, pixel393, pixel130, pixel684, pixel717, pixel179, pixel571, pixel207, pixel371, pixel417, pixel83, pixel255, pixel459, pixel666, pixel540, pixel336, pixel732, pixel377, pixel577, pixel283, pixel396, pixel133, pixel403, pixel639, pixel581, pixel721, pixel543, pixel461, pixel88, pixel692, pixel108, pixel389, pixel686, pixel502, pixel65, pixel431, pixel409, pixel478, pixel703, pixel111, pixel605, pixel648, pixel622, pixel219, pixel125, pixel652, pixel623, pixel460, pixel676, pixel606, pixel314, pixel630, pixel663, pixel9, pixel593, pixel40, pixel252, pixel358, pixel198, pixel136, pixel494, pixel725, pixel641, pixel238, pixel33, pixel532, pixel750, pixel477, pixel542, pixel313, pixel734, pixel76, pixel662, pixel200, pixel82, pixel28, pixel370, pixel520, pixel490, pixel521, pixel784, pixel68, pixel23, pixel186, pixel132, pixel627, pixel463, pixel227, pixel291, pixel189, pixel231, pixel176, pixel535, pixel668, pixel678, pixel294, pixel267, pixel44, pixel651, pixel513, pixel239, pixel475, pixel514, pixel531, pixel220, pixel321, pixel51, pixel733, pixel156, pixel210, pixel628, pixel326, pixel782, pixel47, pixel148, pixel688, pixel335, pixel621, pixel144, pixel22, pixel307, pixel233, pixel93, pixel705, pixel268, pixel192, pixel229, pixel615, pixel43, pixel60, pixel63, pixel191, pixel145, pixel304, pixel332, pixel598, pixel655, pixel719, pixel742, pixel107, pixel263, pixel356, pixel740, pixel429, pixel235, pixel384, pixel224, pixel19, pixel232, pixel756, pixel423, pixel529, pixel568, pixel713, pixel20, pixel119, pixel69, pixel442, pixel609, pixel774, pixel619, pixel213 in input data\ntraining data did not have the following fields: 298, 468, 641, 44, 140, 51, 323, 246, 745, 263, 252, 659, 663, 341, 657, 572, 312, 473, 431, 101, 187, 489, 137, 491, 735, 31, 481, 64, 71, 90, 145, 345, 452, 268, 691, 783, 470, 378, 496, 592, 11, 500, 731, 146, 732, 475, 535, 206, 319, 778, 507, 518, 571, 423, 618, 747, 435, 46, 362, 560, 122, 669, 543, 84, 121, 248, 654, 164, 410, 613, 47, 79, 42, 247, 308, 594, 397, 225, 465, 484, 670, 428, 276, 545, 433, 579, 147, 482, 608, 704, 265, 463, 182, 102, 462, 212, 392, 352, 56, 293, 421, 38, 17, 597, 719, 551, 537, 708, 509, 151, 635, 504, 529, 94, 395, 602, 774, 779, 237, 513, 236, 184, 497, 750, 324, 70, 721, 580, 110, 4, 662, 239, 710, 619, 9, 399, 416, 143, 255, 93, 652, 359, 253, 531, 224, 379, 598, 148, 35, 374, 727, 675, 458, 183, 632, 767, 630, 55, 589, 180, 205, 479, 400, 36, 586, 106, 375, 440, 559, 208, 251, 202, 536, 174, 278, 177, 372, 695, 765, 769, 289, 2, 6, 398, 425, 457, 404, 587, 777, 764, 616, 689, 66, 582, 132, 681, 29, 108, 746, 92, 367, 171, 665, 192, 664, 627, 684, 771, 5, 653, 711, 544, 307, 568, 18, 553, 539, 756, 739, 558, 384, 326, 649, 775, 111, 259, 306, 170, 321, 644, 660, 688, 344, 759, 439, 776, 215, 591, 685, 347, 549, 209, 492, 136, 130, 100, 709, 371, 37, 157, 722, 476, 361, 385, 131, 261, 603, 28, 698, 103, 305, 542, 112, 257, 309, 340, 493, 350, 426, 126, 301, 437, 227, 631, 782, 483, 201, 338, 501, 737, 754, 510, 624, 526, 525, 524, 533, 48, 226, 144, 115, 152, 561, 628, 279, 211, 472, 254, 240, 295, 455, 738, 322, 574, 235, 99, 172, 381, 169, 499, 216, 292, 346, 459, 314, 61, 30, 498, 554, 40, 186, 142, 508, 581, 393, 625, 286, 383, 730, 601, 43, 358, 523, 612, 302, 179, 717, 68, 188, 194, 267, 658, 720, 636, 58, 272, 281, 325, 673, 116, 780, 683, 105, 363, 69, 24, 32, 615, 471, 766, 19, 195, 10, 274, 450, 736, 677, 85, 505, 124, 387, 76, 149, 16, 588, 693, 277, 377, 487, 303, 570, 573, 369, 406, 578, 418, 141, 637, 73, 368, 173, 394, 577, 694, 349, 449, 128, 65, 517, 600, 502, 438, 223, 490, 310, 622, 762, 315, 611, 20, 161, 469, 733, 667, 113, 156, 605, 360, 198, 207, 755, 327, 380, 297, 299, 80, 456, 486, 366, 528, 599, 353, 718, 585, 59, 728, 222, 734, 196, 336, 420, 330, 503, 485, 342, 445, 506, 97, 520, 193, 318, 74, 724, 91, 49, 564, 444, 107, 23, 155, 389, 467, 382, 7, 133, 453, 419, 191, 538, 661, 626, 781, 758, 557, 52, 567, 27, 488, 726, 370, 770, 82, 723, 162, 221, 34, 104, 114, 77, 495, 519, 646, 402, 740, 595, 415, 680, 134, 288, 687, 751, 3, 427, 390, 696, 291, 39, 575, 109, 217, 446, 165, 95, 22, 166, 552, 118, 316, 436, 26, 75, 511, 742, 21, 262, 556, 638, 623, 666, 712, 335, 703, 443, 407, 411, 332, 701, 200, 123, 331, 583, 13, 269, 682, 238, 60, 178, 210, 593, 287, 320, 464, 391, 203, 692, 768, 86, 477, 606, 668, 707, 199, 729, 639, 773, 160, 743, 67, 228, 294, 725, 474, 386, 167, 244, 633, 466, 15, 749, 150, 78, 584, 676, 590, 14, 548, 220, 541, 555, 429, 283, 715, 204, 540, 617, 376, 355, 671, 245, 412, 189, 748, 563, 679, 273, 280, 614, 163, 642, 700, 311, 645, 434, 354, 232, 643, 96, 83, 396, 515, 546, 328, 647, 12, 337, 213, 334, 648, 621, 271, 285, 357, 454, 460, 451, 98, 348, 364, 757, 218, 290, 54, 702, 313, 190, 234, 266, 63, 270, 413, 154, 138, 512, 596, 339, 424, 153, 8, 181, 772, 744, 87, 0, 478, 135, 230, 275, 296, 516, 655, 250, 409, 514, 494, 229, 231, 405, 530, 62, 351, 284, 752, 741, 562, 656, 41, 753, 159, 566, 686, 401, 569, 50, 125, 120, 761, 417, 408, 480, 714, 197, 422, 607, 651, 158, 356, 532, 176, 373, 300, 521, 264, 343, 634, 304, 45, 388, 697, 33, 127, 610, 139, 317, 53, 547, 550, 763, 760, 705, 690, 214, 713, 461, 25, 119, 242, 672, 576, 88, 249, 522, 260, 620, 185, 81, 650, 57, 565, 333, 258, 233, 442, 365, 447, 168, 72, 219, 241, 256, 448, 629, 716, 699, 1, 329, 527, 640, 678, 432, 282, 674, 175, 129, 403, 441, 609, 430, 243, 117, 414, 706, 534, 604, 89"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=4,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=-1,\n",
    "    )\n",
    "xgb.fit(X, y)\n",
    "\n",
    "new_y_pred = xgb.predict(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "\n",
    "# 각 특성의 이름을 조정\n",
    "for i in range(1,785) :\n",
    "    index.append(\"pixel\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6', 'pixel7', 'pixel8', 'pixel9', 'pixel10', 'pixel11', 'pixel12', 'pixel13', 'pixel14', 'pixel15', 'pixel16', 'pixel17', 'pixel18', 'pixel19', 'pixel20', 'pixel21', 'pixel22', 'pixel23', 'pixel24', 'pixel25', 'pixel26', 'pixel27', 'pixel28', 'pixel29', 'pixel30', 'pixel31', 'pixel32', 'pixel33', 'pixel34', 'pixel35', 'pixel36', 'pixel37', 'pixel38', 'pixel39', 'pixel40', 'pixel41', 'pixel42', 'pixel43', 'pixel44', 'pixel45', 'pixel46', 'pixel47', 'pixel48', 'pixel49', 'pixel50', 'pixel51', 'pixel52', 'pixel53', 'pixel54', 'pixel55', 'pixel56', 'pixel57', 'pixel58', 'pixel59', 'pixel60', 'pixel61', 'pixel62', 'pixel63', 'pixel64', 'pixel65', 'pixel66', 'pixel67', 'pixel68', 'pixel69', 'pixel70', 'pixel71', 'pixel72', 'pixel73', 'pixel74', 'pixel75', 'pixel76', 'pixel77', 'pixel78', 'pixel79', 'pixel80', 'pixel81', 'pixel82', 'pixel83', 'pixel84', 'pixel85', 'pixel86', 'pixel87', 'pixel88', 'pixel89', 'pixel90', 'pixel91', 'pixel92', 'pixel93', 'pixel94', 'pixel95', 'pixel96', 'pixel97', 'pixel98', 'pixel99', 'pixel100', 'pixel101', 'pixel102', 'pixel103', 'pixel104', 'pixel105', 'pixel106', 'pixel107', 'pixel108', 'pixel109', 'pixel110', 'pixel111', 'pixel112', 'pixel113', 'pixel114', 'pixel115', 'pixel116', 'pixel117', 'pixel118', 'pixel119', 'pixel120', 'pixel121', 'pixel122', 'pixel123', 'pixel124', 'pixel125', 'pixel126', 'pixel127', 'pixel128', 'pixel129', 'pixel130', 'pixel131', 'pixel132', 'pixel133', 'pixel134', 'pixel135', 'pixel136', 'pixel137', 'pixel138', 'pixel139', 'pixel140', 'pixel141', 'pixel142', 'pixel143', 'pixel144', 'pixel145', 'pixel146', 'pixel147', 'pixel148', 'pixel149', 'pixel150', 'pixel151', 'pixel152', 'pixel153', 'pixel154', 'pixel155', 'pixel156', 'pixel157', 'pixel158', 'pixel159', 'pixel160', 'pixel161', 'pixel162', 'pixel163', 'pixel164', 'pixel165', 'pixel166', 'pixel167', 'pixel168', 'pixel169', 'pixel170', 'pixel171', 'pixel172', 'pixel173', 'pixel174', 'pixel175', 'pixel176', 'pixel177', 'pixel178', 'pixel179', 'pixel180', 'pixel181', 'pixel182', 'pixel183', 'pixel184', 'pixel185', 'pixel186', 'pixel187', 'pixel188', 'pixel189', 'pixel190', 'pixel191', 'pixel192', 'pixel193', 'pixel194', 'pixel195', 'pixel196', 'pixel197', 'pixel198', 'pixel199', 'pixel200', 'pixel201', 'pixel202', 'pixel203', 'pixel204', 'pixel205', 'pixel206', 'pixel207', 'pixel208', 'pixel209', 'pixel210', 'pixel211', 'pixel212', 'pixel213', 'pixel214', 'pixel215', 'pixel216', 'pixel217', 'pixel218', 'pixel219', 'pixel220', 'pixel221', 'pixel222', 'pixel223', 'pixel224', 'pixel225', 'pixel226', 'pixel227', 'pixel228', 'pixel229', 'pixel230', 'pixel231', 'pixel232', 'pixel233', 'pixel234', 'pixel235', 'pixel236', 'pixel237', 'pixel238', 'pixel239', 'pixel240', 'pixel241', 'pixel242', 'pixel243', 'pixel244', 'pixel245', 'pixel246', 'pixel247', 'pixel248', 'pixel249', 'pixel250', 'pixel251', 'pixel252', 'pixel253', 'pixel254', 'pixel255', 'pixel256', 'pixel257', 'pixel258', 'pixel259', 'pixel260', 'pixel261', 'pixel262', 'pixel263', 'pixel264', 'pixel265', 'pixel266', 'pixel267', 'pixel268', 'pixel269', 'pixel270', 'pixel271', 'pixel272', 'pixel273', 'pixel274', 'pixel275', 'pixel276', 'pixel277', 'pixel278', 'pixel279', 'pixel280', 'pixel281', 'pixel282', 'pixel283', 'pixel284', 'pixel285', 'pixel286', 'pixel287', 'pixel288', 'pixel289', 'pixel290', 'pixel291', 'pixel292', 'pixel293', 'pixel294', 'pixel295', 'pixel296', 'pixel297', 'pixel298', 'pixel299', 'pixel300', 'pixel301', 'pixel302', 'pixel303', 'pixel304', 'pixel305', 'pixel306', 'pixel307', 'pixel308', 'pixel309', 'pixel310', 'pixel311', 'pixel312', 'pixel313', 'pixel314', 'pixel315', 'pixel316', 'pixel317', 'pixel318', 'pixel319', 'pixel320', 'pixel321', 'pixel322', 'pixel323', 'pixel324', 'pixel325', 'pixel326', 'pixel327', 'pixel328', 'pixel329', 'pixel330', 'pixel331', 'pixel332', 'pixel333', 'pixel334', 'pixel335', 'pixel336', 'pixel337', 'pixel338', 'pixel339', 'pixel340', 'pixel341', 'pixel342', 'pixel343', 'pixel344', 'pixel345', 'pixel346', 'pixel347', 'pixel348', 'pixel349', 'pixel350', 'pixel351', 'pixel352', 'pixel353', 'pixel354', 'pixel355', 'pixel356', 'pixel357', 'pixel358', 'pixel359', 'pixel360', 'pixel361', 'pixel362', 'pixel363', 'pixel364', 'pixel365', 'pixel366', 'pixel367', 'pixel368', 'pixel369', 'pixel370', 'pixel371', 'pixel372', 'pixel373', 'pixel374', 'pixel375', 'pixel376', 'pixel377', 'pixel378', 'pixel379', 'pixel380', 'pixel381', 'pixel382', 'pixel383', 'pixel384', 'pixel385', 'pixel386', 'pixel387', 'pixel388', 'pixel389', 'pixel390', 'pixel391', 'pixel392', 'pixel393', 'pixel394', 'pixel395', 'pixel396', 'pixel397', 'pixel398', 'pixel399', 'pixel400', 'pixel401', 'pixel402', 'pixel403', 'pixel404', 'pixel405', 'pixel406', 'pixel407', 'pixel408', 'pixel409', 'pixel410', 'pixel411', 'pixel412', 'pixel413', 'pixel414', 'pixel415', 'pixel416', 'pixel417', 'pixel418', 'pixel419', 'pixel420', 'pixel421', 'pixel422', 'pixel423', 'pixel424', 'pixel425', 'pixel426', 'pixel427', 'pixel428', 'pixel429', 'pixel430', 'pixel431', 'pixel432', 'pixel433', 'pixel434', 'pixel435', 'pixel436', 'pixel437', 'pixel438', 'pixel439', 'pixel440', 'pixel441', 'pixel442', 'pixel443', 'pixel444', 'pixel445', 'pixel446', 'pixel447', 'pixel448', 'pixel449', 'pixel450', 'pixel451', 'pixel452', 'pixel453', 'pixel454', 'pixel455', 'pixel456', 'pixel457', 'pixel458', 'pixel459', 'pixel460', 'pixel461', 'pixel462', 'pixel463', 'pixel464', 'pixel465', 'pixel466', 'pixel467', 'pixel468', 'pixel469', 'pixel470', 'pixel471', 'pixel472', 'pixel473', 'pixel474', 'pixel475', 'pixel476', 'pixel477', 'pixel478', 'pixel479', 'pixel480', 'pixel481', 'pixel482', 'pixel483', 'pixel484', 'pixel485', 'pixel486', 'pixel487', 'pixel488', 'pixel489', 'pixel490', 'pixel491', 'pixel492', 'pixel493', 'pixel494', 'pixel495', 'pixel496', 'pixel497', 'pixel498', 'pixel499', 'pixel500', 'pixel501', 'pixel502', 'pixel503', 'pixel504', 'pixel505', 'pixel506', 'pixel507', 'pixel508', 'pixel509', 'pixel510', 'pixel511', 'pixel512', 'pixel513', 'pixel514', 'pixel515', 'pixel516', 'pixel517', 'pixel518', 'pixel519', 'pixel520', 'pixel521', 'pixel522', 'pixel523', 'pixel524', 'pixel525', 'pixel526', 'pixel527', 'pixel528', 'pixel529', 'pixel530', 'pixel531', 'pixel532', 'pixel533', 'pixel534', 'pixel535', 'pixel536', 'pixel537', 'pixel538', 'pixel539', 'pixel540', 'pixel541', 'pixel542', 'pixel543', 'pixel544', 'pixel545', 'pixel546', 'pixel547', 'pixel548', 'pixel549', 'pixel550', 'pixel551', 'pixel552', 'pixel553', 'pixel554', 'pixel555', 'pixel556', 'pixel557', 'pixel558', 'pixel559', 'pixel560', 'pixel561', 'pixel562', 'pixel563', 'pixel564', 'pixel565', 'pixel566', 'pixel567', 'pixel568', 'pixel569', 'pixel570', 'pixel571', 'pixel572', 'pixel573', 'pixel574', 'pixel575', 'pixel576', 'pixel577', 'pixel578', 'pixel579', 'pixel580', 'pixel581', 'pixel582', 'pixel583', 'pixel584', 'pixel585', 'pixel586', 'pixel587', 'pixel588', 'pixel589', 'pixel590', 'pixel591', 'pixel592', 'pixel593', 'pixel594', 'pixel595', 'pixel596', 'pixel597', 'pixel598', 'pixel599', 'pixel600', 'pixel601', 'pixel602', 'pixel603', 'pixel604', 'pixel605', 'pixel606', 'pixel607', 'pixel608', 'pixel609', 'pixel610', 'pixel611', 'pixel612', 'pixel613', 'pixel614', 'pixel615', 'pixel616', 'pixel617', 'pixel618', 'pixel619', 'pixel620', 'pixel621', 'pixel622', 'pixel623', 'pixel624', 'pixel625', 'pixel626', 'pixel627', 'pixel628', 'pixel629', 'pixel630', 'pixel631', 'pixel632', 'pixel633', 'pixel634', 'pixel635', 'pixel636', 'pixel637', 'pixel638', 'pixel639', 'pixel640', 'pixel641', 'pixel642', 'pixel643', 'pixel644', 'pixel645', 'pixel646', 'pixel647', 'pixel648', 'pixel649', 'pixel650', 'pixel651', 'pixel652', 'pixel653', 'pixel654', 'pixel655', 'pixel656', 'pixel657', 'pixel658', 'pixel659', 'pixel660', 'pixel661', 'pixel662', 'pixel663', 'pixel664', 'pixel665', 'pixel666', 'pixel667', 'pixel668', 'pixel669', 'pixel670', 'pixel671', 'pixel672', 'pixel673', 'pixel674', 'pixel675', 'pixel676', 'pixel677', 'pixel678', 'pixel679', 'pixel680', 'pixel681', 'pixel682', 'pixel683', 'pixel684', 'pixel685', 'pixel686', 'pixel687', 'pixel688', 'pixel689', 'pixel690', 'pixel691', 'pixel692', 'pixel693', 'pixel694', 'pixel695', 'pixel696', 'pixel697', 'pixel698', 'pixel699', 'pixel700', 'pixel701', 'pixel702', 'pixel703', 'pixel704', 'pixel705', 'pixel706', 'pixel707', 'pixel708', 'pixel709', 'pixel710', 'pixel711', 'pixel712', 'pixel713', 'pixel714', 'pixel715', 'pixel716', 'pixel717', 'pixel718', 'pixel719', 'pixel720', 'pixel721', 'pixel722', 'pixel723', 'pixel724', 'pixel725', 'pixel726', 'pixel727', 'pixel728', 'pixel729', 'pixel730', 'pixel731', 'pixel732', 'pixel733', 'pixel734', 'pixel735', 'pixel736', 'pixel737', 'pixel738', 'pixel739', 'pixel740', 'pixel741', 'pixel742', 'pixel743', 'pixel744', 'pixel745', 'pixel746', 'pixel747', 'pixel748', 'pixel749', 'pixel750', 'pixel751', 'pixel752', 'pixel753', 'pixel754', 'pixel755', 'pixel756', 'pixel757', 'pixel758', 'pixel759', 'pixel760', 'pixel761', 'pixel762', 'pixel763', 'pixel764', 'pixel765', 'pixel766', 'pixel767', 'pixel768', 'pixel769', 'pixel770', 'pixel771', 'pixel772', 'pixel773', 'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779', 'pixel780', 'pixel781', 'pixel782', 'pixel783', 'pixel784']\n"
     ]
    }
   ],
   "source": [
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_test.columns = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_pred = xgb.predict(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 0 0 2 0 1 1 1 1 3 6 4 2 6 6 3 3 3 3 3 3 3 3 4 3 5 5 5 5 5 4 4 6 4 2 5 5\n",
      " 5 5 5 8 8 8 8 8 8 8 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "print(new_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(new_y_pred, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 1 0 0 0 1 0 0 0]\n",
      " [0 4 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 3 0 0 0]\n",
      " [0 0 0 5 0 0 0 0 0 0]\n",
      " [0 0 0 4 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 5 0 0 0 0]\n",
      " [0 0 1 0 3 0 1 0 0 0]\n",
      " [0 0 0 0 0 5 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 5 0]\n",
      " [0 0 0 0 0 0 0 0 5 0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(answer, new_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75         5\n",
      "           1       1.00      0.80      0.89         5\n",
      "           2       0.33      0.20      0.25         5\n",
      "           3       0.50      1.00      0.67         5\n",
      "           4       0.20      0.20      0.20         5\n",
      "           5       0.50      1.00      0.67         5\n",
      "           6       0.20      0.20      0.20         5\n",
      "           7       0.00      0.00      0.00         5\n",
      "           8       0.50      1.00      0.67         5\n",
      "           9       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.50        50\n",
      "   macro avg       0.42      0.50      0.43        50\n",
      "weighted avg       0.42      0.50      0.43        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(answer, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. mlp사용\n",
    "\n",
    "- 0.48의 정확도를 보임  \n",
    "- xgb보다 못하는 정확도\n",
    "- xgb와 클래스별로 예측 결과가 다름(정확도는 비슷)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_scaled = new_X_test.divide(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(300, 300, 300, 300, 300, 300, 300),\n",
    "                    alpha=0.0001,\n",
    "                    solver='lbfgs',\n",
    "                    max_iter=200,\n",
    "                    random_state=0)\n",
    "mlp.fit(X_scaled, y)\n",
    "\n",
    "new_y_pred_mlp = mlp.predict(new_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(new_y_pred_mlp, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 0 0 0 2 0 0 0]\n",
      " [0 5 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 4 0 1 0 0 0]\n",
      " [0 1 0 4 0 0 0 0 0 0]\n",
      " [0 0 0 2 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 3 0 1 1 0]\n",
      " [0 0 0 0 5 0 0 0 0 0]\n",
      " [0 0 0 0 0 2 0 1 2 0]\n",
      " [0 0 0 0 0 0 0 0 5 0]\n",
      " [0 0 1 0 2 0 2 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(answer, new_y_pred_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75         5\n",
      "           1       0.83      1.00      0.91         5\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.67      0.80      0.73         5\n",
      "           4       0.21      0.60      0.32         5\n",
      "           5       0.60      0.60      0.60         5\n",
      "           6       0.00      0.00      0.00         5\n",
      "           7       0.50      0.20      0.29         5\n",
      "           8       0.62      1.00      0.77         5\n",
      "           9       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.48        50\n",
      "   macro avg       0.44      0.48      0.44        50\n",
      "weighted avg       0.44      0.48      0.44        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(answer, new_y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 직접 가져온 이미지 적용 결과\n",
    "\n",
    "- 약 0.5의 정확도  \n",
    "- 2번(풀오버), 6번(셔츠), 7번(스니커), 9번(앵클부츠)은 아주 낮은 정확도를, 3번(드레스), 5번(샌들), 8번(가방)은 아주 높은 정확도를 보임  \n",
    "- 위치, 컬러맵(8bit-gray), 각 이미지들의 방향이나 모양을 전체적으로 고려하지 못했기 때문이라고 판단"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
